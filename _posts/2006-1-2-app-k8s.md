---
layout: post
title: "K8s | Docker Compose 安装部署教程"
date:   2024-8-30
tags: 
  - 软件类
comments: true
author: feng6917
---

K8s，懂得都懂，容器编排，部署，扩展，负载均衡等。

<!-- more -->

<h2 id="c-1-0" class="mh1">Mac 部署 Minikube</h2>

1. 安装 Home-brew <https://cloud.tencent.com/developer/article/1853162>

    ```shell
    > /usr/bin/ruby -e "$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/install)"
    ```

2. 安装 Minukube <https://minikube.sigs.k8s.io/docs/start/>

   ```shell
   > brew install minikube
   # 遇到问题: No such file or directory @ rb_sysopen - ...
   # 解决方式：https://zhuanlan.zhihu.com/p/491515480
   ```

3. 查看版本

    ```shell
    > kubectl version -o json # 显示版本信息
    ```

<h2 id="c-2-0" class="mh1">Centos7.9 部署 Kubeadmin</h2>

1. 查看版本号

   ```shell
   >  cat /etc/redhat-release
   ```

2. 添加 IP

   ```shell
   > ip add
   ```

3. 修改主机名称

   ```shell
   > hostnamectl set-hostname k8s-master && bash
   ```

4. 添加 hosts,这里的 IP 是自己服务器 ip

   ```shell
   > ifconfig #查看主机IP地址
     cat >> /etc/hosts << EOF
     x.x.x.x  k8s-master
     EOF
   ```

5. 关闭防火墙,关闭 selinux

   ```shell
   > systemctl stop firewalld
   > systemctl disable firewalld
   > sed -i 's/enforcing/disabled/' /etc/selinux/config # 永久
   > setenforce 0 # 临时
   ```

6. 关闭 swap

   ```shell
   > swapoff -a # 临时
   > sed -i 's/.*swap.*/#&/' /etc/fstab # 永久
   ```

7. 将桥接的 IPv4 流量传递到 iptables 的链

   ```shell
   > cat > /etc/sysctl.d/k8s.conf << EOF
     net.bridge.bridge-nf-call-ip6tables = 1
     net.bridge.bridge-nf-call-iptables = 1
     EOF

   > sysctl --system # 生效
   ```

8. 时间同步

   ```shell
   > yum install ntpdate -y
   > ntpdate time.windows.com
   ```

9. 安装 Docker

   ```shell
   > wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo
   > yum -y install docker-ce-20.10.12-3.el7
   > systemctl enable docker && systemctl start docker && systemctl status docker
   > docker --version

   ```

10. 给 docker 添加加速器

    ```shell
    > cat > /etc/docker/daemon.json << EOF
    {
      "registry-mirrors": ["https://qj799ren.mirror.aliyuncs.com"],
      "exec-opts": ["native.cgroupdriver=systemd"],
      "log-driver": "json-file",
      "log-opts": {
      "max-size": "100m"
    },
      "storage-driver": "overlay2"
    }
    EOF

    > systemctl restart docker

    ```

11. 添加 kubernetes 的 yum 源

    ```shell
    > cat > /etc/yum.repos.d/kubernetes.repo << EOF
      [kubernetes]
      name=Kubernetes
      baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
      enabled=1
      gpgcheck=0
      repo_gpgcheck=0
      gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
      https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
      EOF

    ```

12. 安装 kubeadm，kubelet 和 kubectl

    ```shell
    > yum -y install kubelet-1.21.5-0 kubeadm-1.21.5-0 kubectl-1.21.5-0  #当前时间最新版是v1.21.5固定版本，下面有用
    > systemctl enable kubelet
    ```

13. 部署 Kubernetes Master

    ```shell
    > kubeadm init --apiserver-advertise-address=10.0.4.11  --image-repository registry.aliyuncs.com/google_containers  --kubernetes-version v1.21.5  --service-cidr=10.96.0.0/12  --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=all

    ```

- 参数说明：
  - –apiserver-advertise-address=10.0.4.11 这个参数就是 master 主机的 IP 地址，例如我的 Master 主机的 IP 是：10.0.4.11
  - –image-repository registry.aliyuncs.com/google_containers 这个是镜像地址，由于国外地址无法访问，故使用的阿里云仓库地址：repository registry.aliyuncs.com/google_containers
  - –kubernetes-version=v1.21.5 这个参数是下载的 k8s 软件版本号
  - –service-cidr=10.96.0.0/12 这个参数后的 IP 地址直接就套用 10.96.0.0/12 ,以后安装时也套用即可，不要更改
  - –pod-network-cidr=10.244.0.0/16 k8s 内部的 pod 节点之间网络可以使用的 IP 段，不能和 service-cidr 写一样，如果不知道怎么配，就先用这个 10.244.0.0/16
  - –ignore-preflight-errors=all 添加这个会忽略错误
- 执行语句后，看到如下的信息说明就安装成功了。
      ![img.png](img.png)  

1. 执行如下语句

    ```shell
    > mkdir -p $HOME/.kube
    > sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    > sudo chown $(id -u):$(id -g) $HOME/.kube/config
    > kubectl get nodes    #节点状态为NotReady
    ```

2. 安装 Pod 网络插件（CNI）

    ```shell
    > wget https://docs.projectcalico.org/archive/v3.20/manifests/calico.yaml
    ```

    ```
    ---
    # Source: calico/templates/calico-config.yaml
    # This ConfigMap is used to configure a self-hosted Calico installation.
    kind: ConfigMap
    apiVersion: v1
    metadata:
    name: calico-config
    namespace: kube-system
    data:
    # Typha is disabled.
    typha_service_name: &#34;none&#34;
    # Configure the backend to use.
    calico_backend: &#34;bird&#34;

    # Configure the MTU to use
    veth_mtu: &#34;1440&#34;

    # The CNI network configuration to install on each node.  The special
    # values in this config will be automatically populated.
    cni_network_config: |-
        {
        &#34;name&#34;: &#34;k8s-pod-network&#34;,
        &#34;cniVersion&#34;: &#34;0.3.1&#34;,
        &#34;plugins&#34;: [
            {
            &#34;type&#34;: &#34;calico&#34;,
            &#34;log_level&#34;: &#34;info&#34;,
            &#34;datastore_type&#34;: &#34;kubernetes&#34;,
            &#34;nodename&#34;: &#34;__KUBERNETES_NODE_NAME__&#34;,
            &#34;mtu&#34;: __CNI_MTU__,
            &#34;ipam&#34;: {
                &#34;type&#34;: &#34;calico-ipam&#34;
            },
            &#34;policy&#34;: {
                &#34;type&#34;: &#34;k8s&#34;
            },
            &#34;kubernetes&#34;: {
                &#34;kubeconfig&#34;: &#34;__KUBECONFIG_FILEPATH__&#34;
            }
            },
            {
            &#34;type&#34;: &#34;portmap&#34;,
            &#34;snat&#34;: true,
            &#34;capabilities&#34;: {&#34;portMappings&#34;: true}
            }
        ]
        }

    ---
    # Source: calico/templates/kdd-crds.yaml
    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: felixconfigurations.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: FelixConfiguration
        plural: felixconfigurations
        singular: felixconfiguration
    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: ipamblocks.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: IPAMBlock
        plural: ipamblocks
        singular: ipamblock

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: blockaffinities.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: BlockAffinity
        plural: blockaffinities
        singular: blockaffinity

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: ipamhandles.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: IPAMHandle
        plural: ipamhandles
        singular: ipamhandle

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: ipamconfigs.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: IPAMConfig
        plural: ipamconfigs
        singular: ipamconfig

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: bgppeers.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: BGPPeer
        plural: bgppeers
        singular: bgppeer

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: bgpconfigurations.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: BGPConfiguration
        plural: bgpconfigurations
        singular: bgpconfiguration

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: ippools.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: IPPool
        plural: ippools
        singular: ippool

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: hostendpoints.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: HostEndpoint
        plural: hostendpoints
        singular: hostendpoint

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: clusterinformations.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: ClusterInformation
        plural: clusterinformations
        singular: clusterinformation

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: globalnetworkpolicies.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: GlobalNetworkPolicy
        plural: globalnetworkpolicies
        singular: globalnetworkpolicy

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: globalnetworksets.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: GlobalNetworkSet
        plural: globalnetworksets
        singular: globalnetworkset

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: networkpolicies.crd.projectcalico.org
    spec:
    scope: Namespaced
    group: crd.projectcalico.org
    version: v1
    names:
        kind: NetworkPolicy
        plural: networkpolicies
        singular: networkpolicy

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: networksets.crd.projectcalico.org
    spec:
    scope: Namespaced
    group: crd.projectcalico.org
    version: v1
    names:
        kind: NetworkSet
        plural: networksets
        singular: networkset
    ---
    # Source: calico/templates/rbac.yaml

    # Include a clusterrole for the kube-controllers component,
    # and bind it to the calico-kube-controllers serviceaccount.
    kind: ClusterRole
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: calico-kube-controllers
    rules:
    # Nodes are watched to monitor for deletions.
    - apiGroups: [&#34;&#34;]
        resources:
        - nodes
        verbs:
            - watch
            - list
            - get
        # Pods are queried to check for existence.
        - apiGroups: [&#34;&#34;]
            resources:
            - pods
            verbs:
                - get
            # IPAM resources are manipulated when nodes are deleted.
            - apiGroups: [&#34;crd.projectcalico.org&#34;]
                resources:
                - ippools
                verbs:
                    - list
                - apiGroups: [&#34;crd.projectcalico.org&#34;]
                    resources:
                    - blockaffinities
                    - ipamblocks
                    - ipamhandles
                    verbs:
                        - get
                        - list
                        - create
                        - update
                        - delete
                    # Needs access to update clusterinformations.
                    - apiGroups: [&#34;crd.projectcalico.org&#34;]
                        resources:
                        - clusterinformations
                        verbs:
                            - get
                            - create
                            - update
    ---
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: calico-kube-controllers
    roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: calico-kube-controllers
    subjects:
    - kind: ServiceAccount
        name: calico-kube-controllers
        namespace: kube-system
    ---
    # Include a clusterrole for the calico-node DaemonSet,
    # and bind it to the calico-node serviceaccount.
    kind: ClusterRole
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: calico-node
    rules:
    # The CNI plugin needs to get pods, nodes, and namespaces.
    - apiGroups: [&#34;&#34;]
        resources:
        - pods
        - nodes
        - namespaces
        verbs:
            - get
        - apiGroups: [&#34;&#34;]
            resources:
            - endpoints
            - services
            verbs:
                # Used to discover service IPs for advertisement.
                - watch
                - list
                # Used to discover Typhas.
                - get
            - apiGroups: [&#34;&#34;]
                resources:
                - nodes/status
                verbs:
                    # Needed for clearing NodeNetworkUnavailable flag.
                    - patch
                    # Calico stores some configuration information in node annotations.
                    - update
                # Watch for changes to Kubernetes NetworkPolicies.
                - apiGroups: [&#34;networking.k8s.io&#34;]
                    resources:
                    - networkpolicies
                    verbs:
                        - watch
                        - list
                    # Used by Calico for policy information.
                    - apiGroups: [&#34;&#34;]
                        resources:
                        - pods
                        - namespaces
                        - serviceaccounts
                        verbs:
                            - list
                            - watch
                        # The CNI plugin patches pods/status.
                        - apiGroups: [&#34;&#34;]
                            resources:
                            - pods/status
                            verbs:
                                - patch
                            # Calico monitors various CRDs for config.
                            - apiGroups: [&#34;crd.projectcalico.org&#34;]
                                resources:
                                - globalfelixconfigs
                                - felixconfigurations
                                - bgppeers
                                - globalbgpconfigs
                                - bgpconfigurations
                                - ippools
                                - ipamblocks
                                - globalnetworkpolicies
                                - globalnetworksets
                                - networkpolicies
                                - networksets
                                - clusterinformations
                                - hostendpoints
                                - blockaffinities
                                verbs:
                                    - get
                                    - list
                                    - watch
                                # Calico must create and update some CRDs on startup.
                                - apiGroups: [&#34;crd.projectcalico.org&#34;]
                                    resources:
                                    - ippools
                                    - felixconfigurations
                                    - clusterinformations
                                    verbs:
                                        - create
                                        - update
                                    # Calico stores some configuration information on the node.
                                    - apiGroups: [&#34;&#34;]
                                        resources:
                                        - nodes
                                        verbs:
                                            - get
                                            - list
                                            - watch
                                        # These permissions are only requried for upgrade from v2.6, and can
                                        # be removed after upgrade or on fresh installations.
                                        - apiGroups: [&#34;crd.projectcalico.org&#34;]
                                            resources:
                                            - bgpconfigurations
                                            - bgppeers
                                            verbs:
                                                - create
                                                - update
                                            # These permissions are required for Calico CNI to perform IPAM allocations.
                                            - apiGroups: [&#34;crd.projectcalico.org&#34;]
                                                resources:
                                                - blockaffinities
                                                - ipamblocks
                                                - ipamhandles
                                                verbs:
                                                    - get
                                                    - list
                                                    - create
                                                    - update
                                                    - delete
                                                - apiGroups: [&#34;crd.projectcalico.org&#34;]
                                                    resources:
                                                    - ipamconfigs
                                                    verbs:
                                                        - get
                                                    # Block affinities must also be watchable by confd for route aggregation.
                                                    - apiGroups: [&#34;crd.projectcalico.org&#34;]
                                                        resources:
                                                        - blockaffinities
                                                        verbs:
                                                            - watch
                                                        # The Calico IPAM migration needs to get daemonsets. These permissions can be
                                                        # removed if not upgrading from an installation using host-local IPAM.
                                                        - apiGroups: [&#34;apps&#34;]
                                                            resources:
                                                            - daemonsets
                                                            verbs:
                                                                - get
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
    name: calico-node
    roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: calico-node
    subjects:
    - kind: ServiceAccount
        name: calico-node
        namespace: kube-system

    ---
    # Source: calico/templates/calico-node.yaml
    # This manifest installs the calico-node container, as well
    # as the CNI plugins and network config on
    # each master and worker node in a Kubernetes cluster.
    kind: DaemonSet
    apiVersion: apps/v1
    metadata:
    name: calico-node
    namespace: kube-system
    labels:
        k8s-app: calico-node
    spec:
    selector:
        matchLabels:
        k8s-app: calico-node
    updateStrategy:
        type: RollingUpdate
        rollingUpdate:
        maxUnavailable: 1
    template:
        metadata:
        labels:
            k8s-app: calico-node
        annotations:
            # This, along with the CriticalAddonsOnly toleration below,
            # marks the pod as a critical add-on, ensuring it gets
            # priority scheduling and that its resources are reserved
            # if it ever gets evicted.
            scheduler.alpha.kubernetes.io/critical-pod: &#39;&#39;
        spec:
        nodeSelector:
            beta.kubernetes.io/os: linux
        hostNetwork: true
        tolerations:
            # Make sure calico-node gets scheduled on all nodes.
            - effect: NoSchedule
            operator: Exists
            # Mark the pod as a critical add-on for rescheduling.
            - key: CriticalAddonsOnly
            operator: Exists
            - effect: NoExecute
            operator: Exists
        serviceAccountName: calico-node
        # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a &#34;force
        # deletion&#34;: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
        terminationGracePeriodSeconds: 0
        priorityClassName: system-node-critical
        initContainers:
            # This container performs upgrade from host-local IPAM to calico-ipam.
            # It can be deleted if this is a fresh installation, or if you have already
            # upgraded to use calico-ipam.
            - name: upgrade-ipam
            image: calico/cni:v3.11.3
            command: [&#34;/opt/cni/bin/calico-ipam&#34;, &#34;-upgrade&#34;]
                env:
                - name: KUBERNETES_NODE_NAME
                    valueFrom:
                    fieldRef:
                        fieldPath: spec.nodeName
                - name: CALICO_NETWORKING_BACKEND
                    valueFrom:
                    configMapKeyRef:
                        name: calico-config
                        key: calico_backend
                volumeMounts:
                    - mountPath: /var/lib/cni/networks
                    name: host-local-net-dir
                    - mountPath: /host/opt/cni/bin
                    name: cni-bin-dir
                securityContext:
                    privileged: true
                # This container installs the CNI binaries
                # and CNI network config file on each node.
                - name: install-cni
                    image: calico/cni:v3.11.3
                    command: [&#34;/install-cni.sh&#34;]
                    env:
                        # Name of the CNI config file to create.
                        - name: CNI_CONF_NAME
                        value: &#34;10-calico.conflist&#34;
                        # The CNI network config to install on each node.
                        - name: CNI_NETWORK_CONFIG
                        valueFrom:
                            configMapKeyRef:
                            name: calico-config
                            key: cni_network_config
                        # Set the hostname based on the k8s node name.
                        - name: KUBERNETES_NODE_NAME
                        valueFrom:
                            fieldRef:
                            fieldPath: spec.nodeName
                        # CNI MTU Config variable
                        - name: CNI_MTU
                        valueFrom:
                            configMapKeyRef:
                            name: calico-config
                            key: veth_mtu
                        # Prevents the container from sleeping forever.
                        - name: SLEEP
                        value: &#34;false&#34;
                        volumeMounts:
                        - mountPath: /host/opt/cni/bin
                            name: cni-bin-dir
                        - mountPath: /host/etc/cni/net.d
                            name: cni-net-dir
                        securityContext:
                        privileged: true
                        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes
                        # to communicate with Felix over the Policy Sync API.
                        - name: flexvol-driver
                        image: calico/pod2daemon-flexvol:v3.11.3
                        volumeMounts:
                            - name: flexvol-driver-host
                            mountPath: /host/driver
                        securityContext:
                            privileged: true
                        containers:
                        # Runs calico-node container on each Kubernetes node.  This
                        # container programs network policy and routes on each
                        # host.
                        - name: calico-node
                            image: calico/node:v3.11.3
                            env:
                            # Use Kubernetes API as the backing datastore.
                            - name: DATASTORE_TYPE
                                value: &#34;kubernetes&#34;
                            # Wait for the datastore.
                            - name: WAIT_FOR_DATASTORE
                                value: &#34;true&#34;
                            # Set based on the k8s node name.
                            - name: NODENAME
                                valueFrom:
                                fieldRef:
                                    fieldPath: spec.nodeName
                            # Choose the backend to use.
                            - name: CALICO_NETWORKING_BACKEND
                                valueFrom:
                                configMapKeyRef:
                                    name: calico-config
                                    key: calico_backend
                            # Cluster type to identify the deployment type
                            - name: CLUSTER_TYPE
                                value: &#34;k8s,bgp&#34;
                            # Auto-detect the BGP IP address.
                            - name: IP
                                value: &#34;autodetect&#34;
                            # Enable IPIP
                            - name: CALICO_IPV4POOL_IPIP
                                value: &#34;Always&#34;
                            # Set MTU for tunnel device used if ipip is enabled
                            - name: FELIX_IPINIPMTU
                                valueFrom:
                                configMapKeyRef:
                                    name: calico-config
                                    key: veth_mtu
                            # The default IPv4 pool to create on startup if none exists. Pod IPs will be
                            # chosen from this range. Changing this value after installation will have
                            # no effect. This should fall within &#96;--cluster-cidr&#96;.
                            - name: CALICO_IPV4POOL_CIDR
                                value: &#34;10.244.0.0/16&#34;
                            # Disable file logging so &#96;kubectl logs&#96; works.
                            - name: CALICO_DISABLE_FILE_LOGGING
                                value: &#34;true&#34;
                            # Set Felix endpoint to host default action to ACCEPT.
                            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
                                value: &#34;ACCEPT&#34;
                            # Disable IPv6 on Kubernetes.
                            - name: FELIX_IPV6SUPPORT
                                value: &#34;false&#34;
                            # Set Felix logging to &#34;info&#34;
                            - name: FELIX_LOGSEVERITYSCREEN
                                value: &#34;info&#34;
                            - name: FELIX_HEALTHENABLED
                                value: &#34;true&#34;
                            securityContext:
                            privileged: true
                            resources:
                            requests:
                                cpu: 250m
                            livenessProbe:
                            exec:
                                command:
                                - /bin/calico-node
                                - -felix-live
                                - -bird-live
                            periodSeconds: 10
                            initialDelaySeconds: 10
                            failureThreshold: 6
                            readinessProbe:
                            exec:
                                command:
                                - /bin/calico-node
                                - -felix-ready
                                - -bird-ready
                            periodSeconds: 10
                            volumeMounts:
                            - mountPath: /lib/modules
                                name: lib-modules
                                readOnly: true
                            - mountPath: /run/xtables.lock
                                name: xtables-lock
                                readOnly: false
                            - mountPath: /var/run/calico
                                name: var-run-calico
                                readOnly: false
                            - mountPath: /var/lib/calico
                                name: var-lib-calico
                                readOnly: false
                            - name: policysync
                                mountPath: /var/run/nodeagent
                        volumes:
                        # Used by calico-node.
                        - name: lib-modules
                            hostPath:
                            path: /lib/modules
                        - name: var-run-calico
                            hostPath:
                            path: /var/run/calico
                        - name: var-lib-calico
                            hostPath:
                            path: /var/lib/calico
                        - name: xtables-lock
                            hostPath:
                            path: /run/xtables.lock
                            type: FileOrCreate
                        # Used to install CNI.
                        - name: cni-bin-dir
                            hostPath:
                            path: /opt/cni/bin
                        - name: cni-net-dir
                            hostPath:
                            path: /etc/cni/net.d
                        # Mount in the directory for host-local IPAM allocations. This is
                        # used when upgrading from host-local to calico-ipam, and can be removed
                        # if not using the upgrade-ipam init container.
                        - name: host-local-net-dir
                            hostPath:
                            path: /var/lib/cni/networks
                        # Used to create per-pod Unix Domain Sockets
                        - name: policysync
                            hostPath:
                            type: DirectoryOrCreate
                            path: /var/run/nodeagent
                        # Used to install Flex Volume Driver
                        - name: flexvol-driver-host
                            hostPath:
                            type: DirectoryOrCreate
                            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
    ---

    apiVersion: v1
    kind: ServiceAccount
    metadata:
    name: calico-node
    namespace: kube-system

    ---
    # Source: calico/templates/calico-kube-controllers.yaml

    # See https://github.com/projectcalico/kube-controllers
    apiVersion: apps/v1
    kind: Deployment
    metadata:
    name: calico-kube-controllers
    namespace: kube-system
    labels:
        k8s-app: calico-kube-controllers
    spec:
    # The controllers can only have a single active instance.
    replicas: 1
    selector:
        matchLabels:
        k8s-app: calico-kube-controllers
    strategy:
        type: Recreate
    template:
        metadata:
        name: calico-kube-controllers
        namespace: kube-system
        labels:
            k8s-app: calico-kube-controllers
        annotations:
            scheduler.alpha.kubernetes.io/critical-pod: &#39;&#39;
        spec:
        nodeSelector:
            beta.kubernetes.io/os: linux
        tolerations:
            # Mark the pod as a critical add-on for rescheduling.
            - key: CriticalAddonsOnly
            operator: Exists
            - key: node-role.kubernetes.io/master
            effect: NoSchedule
        serviceAccountName: calico-kube-controllers
        priorityClassName: system-cluster-critical
        containers:
            - name: calico-kube-controllers
            image: calico/kube-controllers:v3.11.3
            env:
                # Choose which controllers to run.
                - name: ENABLED_CONTROLLERS
                value: node
                - name: DATASTORE_TYPE
                value: kubernetes
            readinessProbe:
                exec:
                command:
                    - /usr/bin/check-status
                    - -r
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
    name: calico-kube-controllers
    namespace: kube-system
    ---
    # Source: calico/templates/calico-etcd-secrets.yaml
    ---
    # Source: calico/templates/calico-typha.yaml
    ---
    # Source: calico/templates/configure-canal.yaml
    EOF
    ```

    注意唯一需要修改的是 CALICO_IPV4POOL_CIDR 对应的 IP，需要与前面 kubeadm init 的 --pod-network-cidr 指定的一样。–pod-network-cidr=10.244.0.0/16

    ```shell
    > kubectl apply -f calico.yaml
    ```

3. 验证网络

    ```shell
    > kubectl get nodes  #看到 Ready说明网络就正常了
    > kubectl get pods -n kube-system  #全部显示Running就是对的
    ```

4. 单集版的 k8s 安装后, 无法部署服务

    ```shell
    # 因为默认master不能部署pod,有污点, 需要去掉污点或者新增一个node，我这里是去除污点。
    > kubectl get node -o yaml | grep taint -A 5 #执行后看到有输出说明有污点
    > kubectl taint nodes --all node-role.kubernetes.io/master-   #执行这句就行，就是取消污点
    ```

5. 安装补全命令的包

    ```shell
    > yum -y install bash-completion  #安装补全命令的包
    > kubectl completion bash
    > source /usr/share/bash-completion/bash_completion
    > kubectl completion bash >/etc/profile.d/kubectl.sh
    > source /etc/profile.d/kubectl.sh
    > cat  >>  /root/.bashrc <<EOF
    source /etc/profile.d/kubectl.sh
    EOF
    ```

6. 测试 kubernetes 集群

    ```shell
    # 在Kubernetes集群中部署一个Nginx：
    > kubectl create deployment nginx --image=nginx
    > kubectl expose deployment nginx --port=80 --type=NodePort
    > kubectl get pods,svc
    # 注意：看到对外暴露的是3xxxx端口,开放安全组端口范围
    ```

> centos 安装 k8s 基本参考 【kubernetes 最新版安装单机版 v1.21.5】，之所以 cpoy 一遍文章内容，主要担心原文丢失！！

---

<h2 id="c-3-0" class="mh1">Centos7.9 部署 K8s 集群</h2>

>
> 该安装包对系统初始化、K8s初始化、部署Docker、部署K8s Master、部署K8s Node 等进行了封装，见闻知意，直接使用即可。
> 通过遵循以下步骤，便可轻松部署Kubernetes集群。如有任何安装问题，请前往bilibil中，搜索：AI-Linker 进行问题留言！之后会更新更多精彩内容。还请多多关注！(文字摘要自安装包部署说明)

##### 安装文档

本文档旨在帮助初级开发者快速安装企业级的Kubernetes（k8s）集群。您无需进行复杂配置，只需简单设置服务器相关信息即可。

##### 安装条件

- 本安装包仅适配 CentOS 7.9、CentOS 7.6 以及 银河麒麟高级服务器操作系统V10 SP2。其他操作系统可能无法成功安装。
- 安装操作系统时请选择标准安装，最小化安装可能缺少必需的组件。
- 安装完成后，请将服务器IP地址设置为静态，以避免重启后IP变动引起的集群不可用问题。

##### 安装步骤

1. **修改配置文件**： 修改install_host 文件，参考如下修改示例:

   ```toml
   # 该示例讲解了，如何搭建一个三节点的k8s集群
   [all] #这里配置所有 机器的名称 和 机器的ip
   k8s-master ansible_host=192.168.211.110
   k8s-node1  ansible_host=192.168.211.120
   k8s-node2  ansible_host=192.168.211.130
   
   [all:vars] # 这里配置机器的账户密码，为了方便，所有机器账户密码需要保持一致
   ansible_ssh_pass=k8s@2024
   ansible_ssh_port=22
   ansible_ssh_user=root
   
   [master] # 这里配置master节点的名称，名称就是上面all中填写的名称
   k8s-master
   
   [node] # 这里配置node节点的名称，名称就是上面all中填写的名称
   k8s-node1
   k8s-node2
   ```

   这里的配置中包括了主节点（master）和工作节点（node）的设置。

2. **执行安装命令**： 在服务器上打开终端，运行以下命令以开始安装：

   ```shell
   ./install.sh
   ```

3. **检查安装状态**： 安装完成后，检查日志中各节点的状态。若显示 `failed=0`，则表示集群安装成功：

   ```shell
   PLAY RECAP********************************************************************
   k8s-master                 : ok=58   changed=56   unreachable=0    failed=0    skipped=5    rescued=0    ignored=1
   k8s-node1                  : ok=55   changed=53   unreachable=0    failed=0    skipped=4    rescued=0    ignored=1
   k8s-node2                  : ok=55   changed=53   unreachable=0    failed=0    skipped=4    rescued=0    ignored=1
   ```

[下载地址](通过百度网盘分享的文件：cluster-installer.zip
链接：<https://pan.baidu.com/s/17szuBvV3McxvC6O81BahjQ?pwd=t474>
提取码：t474)

<h2 id="c-4-0" class="mh1">Helm 部署 Kafka </h2>

1. 镜像下载，可通过链接下载kafka3.5，也可以通过<https://docker.aityp.com/> 搜索kafka、kafka-ui进行下载

2. 创建pv、pvc

   1. 创建pv、pvc yaml 文件，参考如下：

        ```
        apiVersion: v1
        kind: PersistentVolume
        metadata:
        name: kafka
        spec:
        claimRef:
            name: kafka
            namespace: kafka # 自定义命名空间
        capacity:
            storage: 3Ti # 自定义存储空间
        volumeMode: Filesystem
        accessModes:
            - ReadWriteOnce
        persistentVolumeReclaimPolicy: Retain
        storageClassName: local-storage
        local:
            path: /data/localpv/kafka # 自定义存储路径
        nodeAffinity:
            required:
            nodeSelectorTerms:
                - matchExpressions:
                    - key: kubernetes.io/hostname
                    operator: In
                    values:
                        - "k8s-master-7" # 自定义节点名称
        ---
        kind: PersistentVolumeClaim
        apiVersion: v1
        metadata:
        name: kafka
        namespace: kafka # 自定义命名空间
        spec:
        accessModes:
            - ReadWriteOnce
        resources:
            requests:
            storage: 3Ti # 自定义存储空间
        storageClassName: local-storage 
        ```

   2. 创建本地storage

      ```shell
      mkdir -p /data/localpv/kafka
      chmode 777 /data/localpv/kafka
      ```

   3. 执行命令创建pv、pvc

      ```shell
      kubectl apply -f kafka-pv.yaml
      ```

3. 创建kafka、kafka-ui
    1. kafka

        - kafka/Chart.yaml

            ```
            apiVersion: v2
            name: kafka
            description: A Helm chart for Kafka
            type: application
            version: 0.1.0
            appVersion: "1.0"
            ```

        - kafka/templates/services.yaml

            ```
            apiVersion: v1
            kind: Service
            metadata:
            name: {{ .Values.service.name }}
            labels:
                k8s-app: {{ .Release.Name }}
            spec:
            type: {{ .Values.service.type }}
            ports:
                {{- toYaml .Values.service.ports | nindent 4 }}
            selector:
                k8s-app: {{ .Release.Name }}
            ```

        - kafka/templates/statefulsets.yaml

            ```
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
            name: {{ .Release.Name }}
            labels:
                k8s-app: {{ .Release.Name }}
            spec:
            serviceName: {{ .Release.Name }}
            replicas: {{ .Values.statefulSet.replicas }}
            selector:
                matchLabels:
                k8s-app: {{ .Release.Name }}
            template:
                metadata:
                labels:
                    k8s-app: {{ .Release.Name }}
                spec:
                containers:
                    - name: kafka
                    image: {{ .Values.statefulSet.imageRepository }}:{{ .Values.statefulSet.imageVersion }}
                    imagePullPolicy: {{.Values.statefulSet.imagePullPolicy}}
                    command:
                        - sh
                        - -c
                        - |
                        NODE_ID=${POD_NAME##*-} 
                        echo "broker.id=$NODE_ID" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "node.id=$NODE_ID" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        CONTROLLER_QUORUM="" 
                        for i in $(seq 0 $((${KAFKA_REPLICAS}-1))); do 
                            CONTROLLER_QUORUM="${CONTROLLER_QUORUM}${i}@kafka-${i}.kafka:9093," 
                        done 
                        CONTROLLER_QUORUM=$(echo $CONTROLLER_QUORUM | sed 's/,$//') 
                        echo "controller.quorum.voters=$CONTROLLER_QUORUM" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "advertised.listeners=PLAINTEXT://${HOST_IP}:30092" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "num.partitions=12" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "default.replication.factor=${KAFKA_REPLICAS}" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "log.retention.minutes=30" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "message.max.bytes=6000000" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "max.request.bytes=6100000" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "replica.fetch.max.bytes=6200000" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "log.dirs=/bitnami/kafka" >> /opt/bitnami/kafka/config/kraft/server.properties &&  
                        if [ ! -f "/bitnami/kafka/meta.properties" ]; then
                            kafka-storage.sh format -t LelM2dIFQkiUFvXCEcqRWA -c /opt/bitnami/kafka/config/kraft/server.properties 
                        fi &&
                        exec kafka-server-start.sh /opt/bitnami/kafka/config/kraft/server.properties
                    ports:
                        {{- toYaml .Values.statefulSet.ports | nindent 12 }}
                    resources:
                        {{- toYaml .Values.resources | nindent 12 }}
                    env:
                        - name: KAFKA_ENABLE_KRAFT
                        value: "yes"
                        - name: KAFKA_CFG_PROCESS_ROLES
                        value: "broker,controller"
                        - name: KAFKA_CFG_CONTROLLER_LISTENER_NAMES
                        value: "CONTROLLER"
                        - name: KAFKA_CFG_LISTENERS
                        value: "PLAINTEXT://:9092,CONTROLLER://:9093"
                        - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
                        value: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
                        - name: KAFKA_KRAFT_CLUSTER_ID
                        value: "LelM2dIFQkiUFvXCEcqRWA"
                        - name: ALLOW_PLAINTEXT_LISTENER
                        value: "yes"
                        - name: KAFKA_HEAP_OPTS
                        value: -Xmx12G -Xms12G
                        - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
                        value: "true"
                        - name: KAFKA_REPLICAS
                        value: "{{ .Values.statefulSet.replicas }}"
                        - name: HOST_IP
                        valueFrom:
                            fieldRef:
                            fieldPath: status.hostIP
                        - name: POD_NAME
                        valueFrom:
                            fieldRef:
                            fieldPath: metadata.name
                    volumeMounts:
                        - mountPath: /bitnami/kafka
                        name: kafka
                nodeSelector:
                    zhst.com/kafka: "1"
                volumes:
                    - name: kafka
                    persistentVolumeClaim:
                        claimName: kafka
            ```

        - kafka/values.yaml

            ```
            statefulSet:
            replicas: 1
            imageRepository: kafka # 镜像
            imageVersion: 3.5.0 # 镜像 tag
            imagePullPolicy: IfNotPresent
            ports:
            - name: kafka
                containerPort: 9092
                protocol: TCP
            - name: kafka-ctl
                containerPort: 9093
                protocol: TCP

            service:
            name: kafka
            type: NodePort
            ports:
            - port: 9092
                targetPort: 9092
                nodePort: 30092
                protocol: TCP
                name: kafka
            - port: 9093
                targetPort: 9093
                nodePort: 30093
                protocol: TCP
                name: kafka-ctl

            resources:
            requests:
                cpu: 4
                memory: 16Gi
            limits:
                cpu: 8
                memory: 16Gi
            ```
  
    2. kafka-ui

        - kafka-ui/Chart.yaml

            ```
            apiVersion: v2
            name: kafkaui
            description: A Helm chart for Kubernetes

            # A chart can be either an 'application' or a 'library' chart.
            #
            # Application charts are a collection of templates that can be packaged into versioned archives
            # to be deployed.
            #
            # Library charts provide useful utilities or functions for the chart developer. They're included as
            # a dependency of application charts to inject those utilities and functions into the rendering
            # pipeline. Library charts do not define any templates and therefore cannot be deployed.
            type: application

            # This is the chart version. This version number should be incremented each time you make changes
            # to the chart and its templates, including the app version.
            # Versions are expected to follow Semantic Versioning (https://semver.org/)
            version: 0.1.0

            # This is the version number of the application being deployed. This version number should be
            # incremented each time you make changes to the application. Versions are not expected to
            # follow Semantic Versioning. They should reflect the version the application is using.
            # It is recommended to use it with quotes.
            appVersion: "1.16.0"
            ```

        - kafka-ui/templates/deployment.yaml

            ```
            apiVersion: apps/v1
            kind: Deployment
            metadata:
            name: {{.Release.Name}}
            spec:
            replicas: {{.Values.deployment.replicas}}
            selector:
                matchLabels:
                k8s-app: {{.Release.Name}}
            template:
                metadata:
                labels:
                    k8s-app: {{.Release.Name}}
                spec:
                containers:
                - name: kafka-ui
                    image: {{.Values.deployment.imageRepository}}:{{.Values.deployment.imageTag}}
                    imagePullPolicy: {{.Values.deployment.imagePullPolicy}}
                    ports:
                    - containerPort: 8080
                    env:
                    - name: KAFKA_CLUSTERS_0_NAME
                    value: kafkaCluster
                    - name: KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS
                    value: kafka:9092
                    - name: DYNAMIC_CONFIG_ENABLED
                    value: "true"
                    - name: TZ
                    value: Asia/Shanghai
            ```

        - kafka-ui/templates/service.yaml

            ```
            apiVersion: v1
            kind: Service
            metadata:
            name: {{.Release.Name}}
            spec:
            type: {{.Values.service.sepc.type}}
            selector:
                k8s-app: {{.Release.Name}}
            ports:
                {{- toYaml .Values.service.ports | nindent 4 }}
            ```

        - kafka-ui/values.yaml

            ```
            deployment:
            replicas: 1
            imageRepository: kafkaui # 镜像
            imageTag: 53a65 # 镜像版本
            imagePullPolicy: IfNotPresent

            service:
            sepc:
                type: NodePort
            ports:
            - name: http
                port: 8888
                targetPort: 8080
                nodePort: 30080
            ```

4. helmfile 部署服务

    - helmfile.yaml

        ```
        releases:

        # kafka
        - name: kafka
            namespace: kafka
            chart: "./kafka"
            values:
            - "./kafka/values.yaml"
            wait: false
            atomic: false

        # kafka-ui
        - name: kafka-ui
            namespace: kafka
            chart: "./kafka-ui"
            values:
            - "./kafka-ui/values.yaml"
            wait: false
            atomic: false
        ```

    - 执行 创建、移除

        ```
        helmfile -f helmfile.yaml apply
        helmfile -f helmfile.yaml delete
        ```

5. 访问服务

    - 访问地址：<http://localhost:30080>

---

<h2 id="c-5-0" class="mh1">Docker Compose 部署 RocketMQ </h2>

```
version: '3.8'
services:
  namesrv:
    image: apache/rocketmq:5.3.2
    container_name: rmqnamesrv
    ports:
      - 9876:9876
    networks:
      - rocketmq
    command: sh mqnamesrv
  broker:
    image: apache/rocketmq:5.3.2
    container_name: rmqbroker
    ports:
      - 10909:10909
      - 10911:10911
      - 10912:10912
    environment:
      - NAMESRV_ADDR=rmqnamesrv:9876
    depends_on:
      - namesrv
    networks:
      - rocketmq
    command: sh mqbroker
  proxy:
    image: apache/rocketmq:5.3.2
    container_name: rmqproxy
    networks:
      - rocketmq
    depends_on:
      - broker
      - namesrv
    ports:
      - 8080:8080
      - 8081:8081
    restart: on-failure
    environment:
      - NAMESRV_ADDR=rmqnamesrv:9876
    command: sh mqproxy
networks:
  rocketmq:
    driver: bridge
    ipam:
     config:
      - subnet: 172.31.1.0/24

```

---

<h2 id="c-6-0" class="mh1">Docker Compose 部署 MQTT </h2>

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: emqx
  labels:
    app: emqx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: emqx
  template:
    metadata:
      labels:
        app: emqx
    spec:
      containers:
      - name: emqx
        image: emqx/emqx:latest
        imagePullPolicy: Never
        ports:
        - containerPort: 1883   # MQTT 端口
        - containerPort: 8083   # WebSocket 端口
        - containerPort: 8084   # WSS 端口
        - containerPort: 8883   # MQTT SSL 端口
        - containerPort: 18083  # Dashboard 端口
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            memory: "256Mi"
            cpu: "250m"
---
apiVersion: v1
kind: Service
metadata:
  name: emqx
  labels:
    app: emqx
spec:
  type: LoadBalancer
  ports:
  - port: 1883
    targetPort: 1883
    protocol: TCP
    name: mqtt
  - port: 8083
    targetPort: 8083
    protocol: TCP
    name: websocket
  - port: 8084
    targetPort: 8084
    protocol: TCP
    name: wss
  - port: 8883
    targetPort: 8883
    protocol: TCP
    name: mqtt-ssl
  - port: 18083
    targetPort: 18083
    protocol: TCP
    name: dashboard
  selector:
    app: emqx

```

---

<h2 id="c-7-0" class="mh1">Docker Compose 部署 TiDB </h2>

```
services:
  pd0:
    image: registry.xxx.com/library/pd:v6.1.0
    ports:
      - "2379"
    volumes:
      - ./config/pd.toml:/pd.toml:ro
      - ./data:/data
      - ./logs:/logs
    command:
      - --name=pd0
      - --client-urls=http://0.0.0.0:2379
      - --peer-urls=http://0.0.0.0:2380
      - --advertise-client-urls=http://pd0:2379
      - --advertise-peer-urls=http://pd0:2380
      - --initial-cluster=pd0=http://pd0:2380,pd1=http://pd1:2380,pd2=http://pd2:2380
      - --data-dir=/data/pd0
      - --config=/pd.toml
      - --log-file=/logs/pd0.log
    networks:
      - huiwan
    restart: on-failure

  pd1:
    image: registry.xxx.com/library/pd:v6.1.0
    ports:
      - "2379"
    volumes:
      - ./config/pd.toml:/pd.toml:ro
      - ./data:/data
      - ./logs:/logs
    command:
      - --name=pd1
      - --client-urls=http://0.0.0.0:2379
      - --peer-urls=http://0.0.0.0:2380
      - --advertise-client-urls=http://pd1:2379
      - --advertise-peer-urls=http://pd1:2380
      - --initial-cluster=pd0=http://pd0:2380,pd1=http://pd1:2380,pd2=http://pd2:2380
      - --data-dir=/data/pd1
      - --config=/pd.toml
      - --log-file=/logs/pd1.log
    networks:
      - huiwan
    restart: on-failure

  pd2:
    image: registry.xxx.com/library/pd:v6.1.0
    ports:
      - "2379"
    volumes:
      - ./config/pd.toml:/pd.toml:ro
      - ./data:/data
      - ./logs:/logs
    command:
      - --name=pd2
      - --client-urls=http://0.0.0.0:2379
      - --peer-urls=http://0.0.0.0:2380
      - --advertise-client-urls=http://pd2:2379
      - --advertise-peer-urls=http://pd2:2380
      - --initial-cluster=pd0=http://pd0:2380,pd1=http://pd1:2380,pd2=http://pd2:2380
      - --data-dir=/data/pd2
      - --config=/pd.toml
      - --log-file=/logs/pd2.log
    networks:
      - huiwan
    restart: on-failure

  tikv0:
    image: registry.xxx.com/library/tikv:v6.1.0
    volumes:
      - ./config/tikv.toml:/tikv.toml:ro
      - ./data:/data
      - ./logs:/logs
    command:
      - --addr=0.0.0.0:20160
      - --advertise-addr=tikv0:20160
      - --data-dir=/data/tikv0
      - --pd=pd0:2379,pd1:2379,pd2:2379
      - --config=/tikv.toml
      - --log-file=/logs/tikv0.log
    networks:
      - huiwan
    depends_on:
      - "pd0"
      - "pd1"
      - "pd2"
    restart: on-failure

  tikv1:
    image: registry.xxx.com/library/tikv:v6.1.0
    volumes:
      - ./config/tikv.toml:/tikv.toml:ro
      - ./data:/data
      - ./logs:/logs
    command:
      - --addr=0.0.0.0:20160
      - --advertise-addr=tikv1:20160
      - --data-dir=/data/tikv1
      - --pd=pd0:2379,pd1:2379,pd2:2379
      - --config=/tikv.toml
      - --log-file=/logs/tikv1.log
    networks:
      - huiwan
    depends_on:
      - "pd0"
      - "pd1"
      - "pd2"
    restart: on-failure

  tikv2:
    image: registry.xxx.com/library/tikv:v6.1.0
    volumes:
      - ./config/tikv.toml:/tikv.toml:ro
      - ./data:/data
      - ./logs:/logs
    command:
      - --addr=0.0.0.0:20160
      - --advertise-addr=tikv2:20160
      - --data-dir=/data/tikv2
      - --pd=pd0:2379,pd1:2379,pd2:2379
      - --config=/tikv.toml
      - --log-file=/logs/tikv2.log
    networks:
      - huiwan
    depends_on:
      - "pd0"
      - "pd1"
      - "pd2"
    restart: on-failure

  tidb:
    image: registry.xxx.com/library/tidb:v6.1.0
    expose:
      - "4000"
      - "10080"
    volumes:
      - ./config/tidb.toml:/tidb.toml:ro
      - ./logs:/logs
    command:
      - --store=tikv
      - --path=pd0:2379,pd1:2379,pd2:2379
      - --config=/tidb.toml
      - --log-file=/logs/tidb.log
      - --advertise-address=tidb
    networks:
      - huiwan
    depends_on:
      - "tikv0"
      - "tikv1"
      - "tikv2"
    restart: on-failure
```

---

<h2 id="c-8-0" class="mh1">Docker Compose 部署 MySQL </h2>

```
services:
  mysql57:
    image: registry.xxx.com/video-analysis/mysql:5.7.44
    restart: always
    container_name: mysql57
    hostname: mysql57
    environment:
      MYSQL_ROOT_PASSWORD: 2755ba37b72
      TZ: Asia/Shanghai
    ports:
      - 13306:3306
    volumes:
      - ./mysql_data:/var/lib/mysql
      - /home/mysql/conf/my.cnf:/etc/mysql/my.cnf
    command:
      --max_connections=1000
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_general_ci
      --default-authentication-plugin=mysql_native_password
    networks:
      - huiwan
    healthcheck:
      test: ["CMD-SHELL", "curl --silent localhost:3306 >/dev/null || exit 1"]
      interval: 10s
      timeout: 10s

# 连接外部网络
networks:
  huiwan:
    external: true
```

---

<h2 id="c-9-0" class="mh1">Docker Compose 部署 NSQ </h2>

```
services:
###
  nsqlookupd:
    image: ${NSQ_VERSION}
    restart: always
    command: /bin/nsqlookupd
    ports:
      - "4160:4160"
      - "4161:4161"
    networks:
      fengren:
        ipv4_address: 172.18.0.59
  nsqd:
    image: ${NSQ_VERSION}
    restart: always
    command: /bin/nsqd -data-path=/data --lookupd-tcp-address=nsqlookupd:4160 --broadcast-address=nsqd
    depends_on:
      - nsqlookupd
    volumes:
      - ${NSQD_DATA_DIR}:/data
    ports:
      - "4150:4150"
      - "4151:4151"
    networks:
      fengren:
        ipv4_address: 172.18.0.60
  nsqadmin:
    image: ${NSQ_VERSION}
    restart: always
    command: /bin/nsqadmin --lookupd-http-address=nsqlookupd:4161
    depends_on:
      - nsqlookupd
    ports:
      - "4171:4171"
    networks:
      fengren:
        ipv4_address: 172.18.0.61
```

---

<h2 id="c-10-0" class="mh1">Docker Compose 部署 Redis </h2>

```
services:
###
  redis:
    image: ${REDIS_VERSION}
    restart: always
    healthcheck:
      test: ["CMD", "redis-cli"]
    ports:
      - "6379:6379"
      - "26379:26379"
    command:
      - redis-server
      - /data/conf/redis.conf
    volumes:
      - ${REDIS_DATA_DIR}/redis:/data
      - ${CONFIG_DIR}/redis-config:/data/conf
    networks:
      fengren:
        ipv4_address: 172.18.0.62

  redis-sentinel:
    image: ${REDIS_VERSION}
    restart: always
    command:
      - sh
      - -c
      - cp /data/conf/sentinel.conf.tmpl /data/conf/sentinel.conf && redis-sentinel /data/conf/sentinel.conf
    volumes:
      - ${REDIS_DATA_DIR}/redis:/redis-data
      - ${CONFIG_DIR}/redis-config/sentinel.conf:/data/conf/sentinel.conf.tmpl
    networks:
      fengren:
        ipv4_address: 172.18.0.63
    network_mode: service:redis
    depends_on:
      redis:
        condition: service_started
```

---

<h2 id="c-11-0" class="mh1">Docker Compose 部署 Vearch </h2>

<h2 id="c-11-1" class="mh2">vearch 部署 </h2>

```
services:
 ###
  vearch-master:
    image: ${VEARCH_VERSION}
    restart: always
    entrypoint:
      - sh
      - -c
      - cd /vearch && cp config.toml.tpl config.toml &&  getent hosts vearch-master| awk '{print "sed -i s/vearch-master/"$$1"/g config.toml"}'|sh && /vearch/bin/vearch -conf /vearch/config.toml master
      # - cd /vearch && cp config.toml.tpl config.toml &&  getent hosts vearch-master| awk '{print "sed -i s/vearch-master/"$$1"/g config.toml"}'|sh && sleep 10d
    volumes:
      - ${CONFIG_DIR}/vearch-config/config.toml:/vearch/config.toml.tpl
      - /etc/localtime:/etc/localtime
      - ${VEARCH_DATA_DIR}/vearch/master/data:/datas
      - ${VEARCH_DATA_DIR}/vearch/master/logs:/logs
    ports:
      - "8817:8817"
    networks:
      fengren:
        ipv4_address: 172.18.0.64
  vearch-ps:
    image: ${VEARCH_VERSION}
    restart: always
    healthcheck:
      test: ["CMD", "echo", "1"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    entrypoint:
      - sh
      - -c
      - rm -rf /datas/raft/*/* && /vearch/bin/vearch  -conf /vearch/config.toml
        ps
    environment:
      LD_PRELOAD: /vearch/lib/libtcmalloc.so.4
      OMP_WAIT_POLICY: PASSIVE
      OMP_NUM_THREADS: "6"
      TCMALLOC_RELEASE_RATE: "6"
    volumes:
      - ${CONFIG_DIR}/vearch-config/config.toml:/vearch/config.toml
      - /etc/localtime:/etc/localtime
      - ${VEARCH_DATA_DIR}/vearch/ps/data:/datas
    networks:
      fengren:
        ipv4_address: 172.18.0.65
  vearch-router:
    image: ${VEARCH_VERSION}
    restart: always
    healthcheck:
      test: ["CMD", "echo", "1"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    entrypoint:
      - /vearch/bin/vearch
      - -conf
      - /vearch/config.toml
      - router
    environment:
      NO_SOURCE: "1"
      AUTO_GC: "1"
    volumes:
      - ${CONFIG_DIR}/vearch-config/config.toml:/vearch/config.toml
      - /etc/localtime:/etc/localtime
      - ${VEARCH_DATA_DIR}/vearch/router/logs:/datas
      - ${VEARCH_DATA_DIR}/vearch/router/logs:/logs
    networks:
      fengren:
        ipv4_address: 172.18.0.66
```

### vearch 相关操作 基于版本3.3x 文档地址（<https://vearch.readthedocs.io/zh-cn/latest/）>

<h2 id="c-11-2" class="mh2">查看集群中所有库 </h2>

```
curl -XGET http://master_server/list/db
```

<h2 id="c-11-3" class="mh2">查看库信息 </h2>

```
curl -XGET http://master_server/db/$db_name
```

<h2 id="c-11-4" class="mh2">创建库 </h2>

``` bash
# 创建库
curl -XPUT -H "content-type:application/json" -d '{
    "name": "hahah"
}
' 10.102.198.126:8817/db/_create
```

<h2 id="c-11-5" class="mh2">查看表空间 </h2>

```
curl -XGET http://master_server/space/$db_name/$space_name
```

<h2 id="c-11-6" class="mh2">创建表空间 </h2>

``` bash
# face
curl -X PUT 10.102.198.126:8817/space/face/_create \
  -H "Content-Type: application/json" \
  -d '{
    "name": "p_20230907",
    "partition_num": 1,
    "replica_num": 1,
    "properties": {
    "timestamp": {
      "type": "integer",
      "index": true
    },
    "groupid": {
      "type": "integer",
      "index": true
    },
    "objectid": {
      "type": "long",
      "index": false
    },
    "extra": {
      "type": "integer",
      "index": false
    },
    "feature0": {
      "type": "vector",
      "dimension": 512,
      "store_type": "RocksDB",
      "store_param": {
        "cache_size": 1000
      }
    }
 },
    "engine": {
      "name": "gamma",
      "index_size": 100000,
      "metric_type": "InnerProduct",
      "retrieval_type": "IVFPQ",
      "retrieval_param": {
        "metric_type": "InnerProduct",
        "ncentroids": 2048,
        "nsubvector": 64,
        "bucket_init_size": 200,
        "bucket_max_size": 40960000
      }
    }
  }'

# reid_head_attr
curl -X PUT 10.102.198.126:8817/space/reid_head_attr/_create \
-H "Content-Type: application/json" \
-d '{
    "partition_num": 1,
    "replica_num": 1,
    "engine": {
        "name": "gamma",
        "index_size": 100000,
        "metric_type": "L2",
        "retrieval_type": "IVFPQ",
        "retrieval_param": {
            "metric_type": "L2",
            "ncentroids": 2048,
            "nsubvector": 33,
            "bucket_init_size": 200,
            "bucket_max_size": 40960000,
            "pre_train_type": "reid_head_attr"
        },
        "id_type": "Long",
        "load_mode": 0
    },
    "properties": {
        "timestamp": {
            "type": "integer",
            "index": true
        },
        "groupid": {
            "type": "integer",
            "index": true
        },
        "objectid": {
            "type": "long",
            "index": false
        },
        "extra": {
            "type": "integer",
            "index": false
        },
        "feature0": {
            "type": "vector",
            "dimension": 528,
            "store_type": "RocksDB",
            "store_param": {
                "cache_size": 1000
            }
        }
    }
}'
```

<h2 id="c-11-7" class="mh2">id查询 </h2>

```
curl -XGET http://router_server/$db_name/$space_name/$id
```

<h2 id="c-11-8" class="mh2">数据检索 </h2>

``` bash
curl -H "content-type: application/json" -XPOST 10.101.82.90:9001/face/p_20250909/_search -d\
'{
    "query": {
      "sum": [
        {
          "field": "feature0",
          "feature": [-0.073917344,-0.0031729278,0.06760357,0.0006312169,-0.009868479,0.026435718,-0.005569467,0.019018317,-0.030157251,0.05017653,-0.012268227,-0.041167855,0.044478737,-0.021738887,0.05482203,-0.021918546,0.03408411,0.0044337576,-0.017029222,-0.005162023,-0.069194846,0.010510122,0.030901557,0.021867216,0.016503075,-0.019082481,0.035393063,0.054616705,-0.092961326,0.041347515,0.03837029,-0.033211473,0.0062111104,0.067911565,0.035213403,-0.082387045,-0.0891628,0.024164299,0.022342032,-0.051665146,0.013384686,0.046916984,-0.034905415,0.077561885,-0.05240945,-0.020160442,-0.07299338,0.05646464,-0.021790218,0.024331126,-0.026076397,-0.0042893877,-0.03256983,0.07540596,0.04186083,-0.03547006,-0.011126101,-0.02126407,0.037625983,-0.12709677,0.08731487,-0.07212074,0.12545416,0.043580435,-0.011530336,0.025948068,-0.00060956145,-0.012011569,-0.0049214065,-0.042502474,0.05138282,0.004260514,0.01840234,0.037805643,-0.03305748,0.012396555,0.07314737,0.028668638,-0.031440537,0.059390534,-0.004687207,-0.05343608,0.045633696,0.0038113631,0.07704857,0.048662253,-0.06062249,-0.023176167,0.041424513,0.053590078,0.042322814,0.0051812725,0.054873362,0.026012233,-0.013692675,-0.05856923,-0.075970605,0.11621449,-0.021649057,-0.03354513,0.03310881,-0.019557297,-0.015810098,-0.035239067,-0.003939692,-0.045454036,0.006069949,0.005569467,-0.035855047,0.01724738,-0.052204125,-0.021110076,0.0074045677,-0.013718341,-0.009175504,-0.0072441567,0.06719292,-0.005883872,-0.0735067,0.0061950693,-0.053230755,-0.08649356,-0.022585856,-0.0029692058,-0.025717078,-0.02823232,-0.0034937495,-0.08510761,-0.06904086,-0.0036284947,-0.008572359,-0.04596735,0.06940018,-0.048533924,-0.058723226,-0.040192556,-0.0054347217,0.008803351,-0.05579733,-0.03811363,-0.021366732,-0.10584553,-0.024780277,-0.00817454,0.0029676019,0.09958309,-0.023381494,-0.06380504,0.020596761,0.0143214855,-0.020378603,0.07027281,-0.05959586,0.019557297,0.024818776,0.04524871,-0.07402001,-0.036471024,0.005088234,0.052820105,0.03541873,0.0012191229,-0.02256019,-0.017221715,-0.021559225,0.006012201,0.011395591,-0.0028793758,-0.027231356,-0.027821667,0.07222341,-0.040269554,-0.056259315,-0.019274974,-0.049971208,0.098351136,0.04881625,0.053487413,-0.0195188,-0.028514642,-0.027436681,-0.045993015,0.046198342,0.026743706,-0.007641976,0.05163948,0.012101399,-0.00862369,0.030901557,0.018582,0.05338475,-0.047455963,-0.019339139,-0.02081492,-0.012486385,-0.053282086,0.007725389,0.022727018,-0.027077362,0.043708764,0.03480275,-0.026820704,-0.022637187,0.013795339,0.0014886132,0.036163036,-0.06986216,-0.03957658,0.012961201,0.011857575,-0.0120308185,0.0014436982,0.007199242,0.05156248,0.012178396,-0.017324379,0.03210785,0.054308716,0.06996482,0.03739499,-0.09901845,0.01733721,0.02581974,0.040859867,0.026333055,0.046429332,-0.010830944,0.008540276,-0.011556002,0.076329924,0.0101443855,0.07586794,-0.0017581036,0.007776721,0.020558262,-0.046839986,0.066320285,0.011652248,-0.043811426,0.029335948,0.053076763,-0.010266298,0.0009039155,0.03300615,-0.020827752,0.0052678944,-0.06513966,0.07283939,0.06647428,-0.0072377403,-0.015514943,0.014590976,-0.08921413,0.01715755,-0.030259915,-0.018787326,0.040859867,0.036111705,0.006172612,0.09090807,0.072069414,0.032544166,-0.04399109,-0.12391422,-0.009515575,-0.06590963,0.055232685,0.030696232,-0.025088266,0.06673094,-0.0640617,-0.084132314,0.05061285,-0.0013955749,0.06852754,-0.03364779,0.05243512,-0.0034199606,0.0066987597,0.08854682,0.047224972,-0.016400412,-0.061751783,0.01921081,-0.045839023,0.013872336,-0.041398846,-0.01058712,-0.030234247,0.053538743,0.013140862,-0.03739499,0.0106641175,0.004347136,0.075046636,0.04897024,0.032672495,-0.061443795,0.0039076097,-0.02162339,0.012159147,-0.009393663,-0.047866616,-0.025126765,-0.058415238,-0.021045912,0.031209547,-0.022945177,-0.046070013,0.028283652,-0.063086405,-0.028488977,-0.0004900553,0.06719292,-0.073198706,-0.03829329,-0.0427078,0.06385638,-0.008309285,0.0427078,-0.035085075,0.044555735,0.010818112,-0.034469098,0.023484157,-0.054514043,-0.08449163,0.041244853,0.121450305,0.012293892,0.1123133,0.07740789,0.06786023,0.048662253,0.03256983,-0.029849261,0.056259315,0.007821636,0.045941684,-0.02841198,0.023189,0.04999687,-0.0002309917,-0.045762025,0.019172313,-0.06200844,0.083619,-0.044658396,-0.06478034,-0.05025353,-0.008424781,-0.009284584,-0.011639415,0.031876855,0.07591928,-0.017876191,0.024959937,-0.033160143,0.042476807,0.044376075,-0.046172675,-0.016772565,-0.056618635,0.041244853,-0.0881875,0.041527174,-0.027719004,-0.053590078,-0.0213539,0.0023788938,-0.048277266,-0.015399448,-0.026281724,0.06786023,0.04157851,-0.059749857,0.019557297,0.051357158,-0.035085075,0.007224907,0.04301579,-0.013410352,0.11847308,-0.020506931,0.006249609,0.031722862,0.026769372,0.006560806,-0.034956746,-0.03606037,0.016490242,0.071350776,0.0043150536,-0.051870473,-0.030054588,-0.028950961,-0.0069040856,0.050407525,-0.04640367,-0.008347783,-0.06416436,-0.02560158,-0.019967949,-0.11652248,-0.03693301,0.004485089,0.013859502,-0.0036252865,0.029951924,0.0027478389,-0.022521691,0.02327883,-0.12627546,0.012370889,-0.019364804,0.0040231054,0.008277203,-0.08669889,0.037138335,-0.031645864,0.011209514,0.046198342,-0.0013795338,0.05959586,-0.006839921,-0.05954453,-0.007847302,-0.008521028,0.0050721928,-0.0010859818,-0.077921204,0.016195085,-0.0123131415,0.024639115,-0.066679604,0.0031296168,-0.00090231135,-0.022059709,-0.043221116,0.032929152,0.009348747,0.013974998,0.07232607,0.016323414,0.04488939,0.037420657,-0.003971774,-0.024202798,-0.031568866,0.04640367,0.004379218,-0.035008077,0.018299676,-0.092499346,-0.014051995,0.03310881,-0.021122908,-0.064883,-0.009913394,-0.098043144,-0.0059929513,0.070118815,0.03811363,-0.010170052,-0.00060795736,-0.04306712,-0.054616705,-0.034648757,-0.03693301,0.05826124,-0.05369274,-0.010863027,0.048174605,0.017504038,-0.027616343,-0.031132549,-0.056618635,0.08243837,-0.003057432,-0.051023502,0.056310646,-0.07961514,0.02497277,-0.02358682,0.0026114895,-0.026063565,-0.030388242,-0.042245816,-0.08110376,0.01688806,-0.014000664,0.03390445,0.027590677,-0.0268977,0.012813623,0.07176142,-0.024356792,0.025524585,-0.024754612,0.08233571,0.05610532,-0.03547006,0.009015093,-0.016836729,-0.016605737,-0.044299077,-0.018351007,0.022573022,-0.03346813,0.003277195,0.0026949032,-0.00068655866]
        }
      ]
    },
    "size": 3,
    "fields": ["_id"] 
}'
```

---

<h2 id="c-12-0" class="mh1">Docker Compose 部署 MINIO </h2>

<h2 id="c-12-1" class="mh2">minio 部署 </h2>

```
services:
###
  minio:
    image: ${MINIO_V3_VERSION}
    restart: always
    command:
      # - /bin/sh
      # - -ce
      # - /usr/bin/docker-entrypoint.sh
      - minio
      - -S
      - /etc/minio/certs
      - server
      - /minio-data
    healthcheck:
      test: ["CMD", "curl", "127.0.0.1:9000"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    ports:
      - "32633:9000"
    environment:
      MINIO_ACCESS_KEY: admin
      MINIO_SECRET_KEY: 4F3RLWHOXW7CZFKVID9N9DQW
      MINIO_BROWSER: "on"
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_STORAGE_CLASS_RRS: EC:2
      MINIO_STORAGE_CLASS_STANDARD: EC:2
    volumes:
      - ${MINIO_DATA_DIR}/minio:/minio-data
    networks:
      fengren:
        ipv4_address: 172.18.0.67
```

<h2 id="c-12-2" class="mh2">权限初始化 </h2>

##### 2.1 初始化文件 `docker compose up -f minio-init.yaml -d`

``` minio-init.yaml
version: '3.9'
services:
  minio-mc:
    image: registry.xxx.com/video-analysis/minio/mc:RELEASE.2021-05-26T19-19-26Z
    entrypoint:
        - sh
        - -c
        - |
          cp /config/* /tmp/
          chmod +x /tmp/initialize
          chmod +x /tmp/makeuser
          /tmp/initialize && /tmp/makeuser
    environment:
      MINIO_ENDPOINT: minio
      MINIO_PORT: "9000"
    volumes:
      - ../config/minio-config:/config
```

##### 2.2 关联文件

ps： `基础文件（账号密码）`

- accesskey > root(账号)
- secretkey > root(密码)

###### 2.2.1 initialize

```
#!/bin/bash
set -e ; # Have script exit in the event of a failed command.
MC_CONFIG_DIR="/etc/minio/mc/"
MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
MINIO_ENDPOINT=172.17.0.1
MINIO_PORT=32633
# connectToMinio
# Use a check-sleep-check loop to wait for Minio service to be available
connectToMinio() {
  SCHEME=$1
  ATTEMPTS=0 ; LIMIT=599 ; # Allow 600 attempts
  set -e ; # fail if we can't read the keys.
  ACCESS=$(cat /config/accesskey) ; SECRET=$(cat /config/secretkey) ;
  set +e ; # The connections to minio are allowed to fail.
  echo "Connecting to Minio server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
  MC_COMMAND="${MC} config host add myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
  $MC_COMMAND ;
  STATUS=$? ;
  until [ $STATUS = 0 ]
  do
    ATTEMPTS=`expr $ATTEMPTS + 1` ;
    echo \"Failed attempts: $ATTEMPTS\" ;
    if [ $ATTEMPTS -gt $LIMIT ]; then
      exit 1 ;
    fi ;
    sleep 2 ; # 1 second intervals between attempts
    $MC_COMMAND ;
    STATUS=$? ;
  done ;
  set -e ; # reset `e` as active
  return 0
}

# checkBucketExists ($bucket)
# Check if the bucket exists, by using the exit code of `mc ls`
checkBucketExists() {
  BUCKET=$1
  CMD=$(${MC} ls myminio/$BUCKET > /dev/null 2>&1)
  return $?
}

# createBucket ($bucket, $policy, $purge)
# Ensure bucket exists, purging if asked to
createBucket() {
  BUCKET=$1
  POLICY=$2
  PURGE=$3
  VERSIONING=$4

  # Purge the bucket, if set & exists
  # Since PURGE is user input, check explicitly for `true`
  if [ $PURGE = true ]; then
    if checkBucketExists $BUCKET ; then
      echo "Purging bucket '$BUCKET'."
      set +e ; # don't exit if this fails
      ${MC} rm -r --force myminio/$BUCKET
      set -e ; # reset `e` as active
    else
      echo "Bucket '$BUCKET' does not exist, skipping purge."
    fi
  fi

  # Create the bucket if it does not exist
  if ! checkBucketExists $BUCKET ; then
    echo "Creating bucket '$BUCKET'"
    ${MC} mb myminio/$BUCKET
  else
    echo "Bucket '$BUCKET' already exists."
  fi


  # set versioning for bucket
  if [ ! -z $VERSIONING ] ; then
    if [ $VERSIONING = true ] ; then
        echo "Enabling versioning for '$BUCKET'"
        ${MC} version enable myminio/$BUCKET
    elif [ $VERSIONING = false ] ; then
        echo "Suspending versioning for '$BUCKET'"
        ${MC} version suspend myminio/$BUCKET
    fi
  else
      echo "Bucket '$BUCKET' versioning unchanged."
  fi

  # At this point, the bucket should exist, skip checking for existence
  # Set policy on the bucket
  echo "Setting policy of bucket '$BUCKET' to '$POLICY'."
  ${MC} policy set $POLICY myminio/$BUCKET
}

# Try connecting to Minio instance
scheme=http
connectToMinio $scheme
# Create the buckets
createBucket xxx download false
...
```

###### 2.2.2 makeuser

```
#!/bin/bash
set -e ; # Have script exit in the event of a failed command.
MC_CONFIG_DIR="/etc/minio/mc/"
MC="/usr/bin/mc --insecure --config-dir ${MC_CONFIG_DIR}"
MINIO_ENDPOINT=172.17.0.1
MINIO_PORT=32633
# connectToMinio
# Use a check-sleep-check loop to wait for Minio service to be available
connectToMinio() {
  SCHEME=$1
  ATTEMPTS=0 ; LIMIT=29 ; # Allow 30 attempts
  set -e ; # fail if we can't read the keys.
  ACCESS=$(cat /config/accesskey) ; SECRET=$(cat /config/secretkey) ;
  set +e ; # The connections to minio are allowed to fail.
  echo "Connecting to Minio server: $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT" ;
  MC_COMMAND="${MC} config host add myminio $SCHEME://$MINIO_ENDPOINT:$MINIO_PORT $ACCESS $SECRET" ;
  $MC_COMMAND ;
  STATUS=$? ;
  until [ $STATUS = 0 ]
  do
    ATTEMPTS=`expr $ATTEMPTS + 1` ;
    echo \"Failed attempts: $ATTEMPTS\" ;
    if [ $ATTEMPTS -gt $LIMIT ]; then
      exit 1 ;
    fi ;
    sleep 2 ; # 1 second intervals between attempts
    $MC_COMMAND ;
    STATUS=$? ;
  done ;
  set -e ; # reset `e` as active
  return 0
}

# checkUserExists ($user)
# Check if the user exists, by using the exit code of `mc ls`
checkUserExists() {
  USER=$1
  CMD=$(${MC} admin user info myminio $USER > /dev/null 2>&1)
  return $?
}

# createBucket ($bucket, $password)
# Ensure bucket exists, purging if asked to
createUser() {
  NAME=$1
  PASSWORD=$2

  # Create the user if it does not exist
  if ! checkUserExists $NAME ; then
    echo "Creating user '$NAME'"
    ${MC} admin user add myminio $NAME $PASSWORD
  else
    echo "User '$NAME' already exists."
  fi
}

# allowBucket ($user, $buckets)
allowBuckets() {
  USER=$1
  BUCKETS=$2
  POLICY="policy_"$USER

  # public,offline,history -> "arn:aws:s3:::public",  "arn:aws:s3:::offline",  "arn:aws:s3:::history"

  B1=$(echo $BUCKETS | sed  's/,/","arn:aws:s3:::/g')
  B1='"arn:aws:s3:::'${B1}
  B1=${B1}'"'

  B2=$(echo $BUCKETS | sed  's#,#/*","arn:aws:s3:::#g')
  B2='"arn:aws:s3:::'${B2}
  B2=${B2}'/*"'

  cp -f /tmp/bucket_readwrite_tpl.json /tmp/$POLICY.json
  sed -i 's#"place_holder_2"#'${B2}'#g' /tmp/$POLICY.json
  sed -i 's#"place_holder_1"#'${B1}'#g' /tmp/$POLICY.json

  setPolicy $USER $POLICY
}

# checkPolicyExists ($policy)
checkPolicyExists() {
  POLICY=$1
  CMD=$(${MC} admin policy info myminio $POLICY > /dev/null 2>&1)
  return $?
}

# allowBucket ($user, $policy)
setPolicy() {
  USER=$1
  POLICY=$2

  # Create the user if it does not exist
  if ! checkPolicyExists $POLICY ; then
    echo "Creating policy '$POLICY'"
    ${MC} admin policy add myminio $POLICY /tmp/$POLICY.json
  else
    echo "Policy '$POLICY' already exists overwite."
    ${MC} admin policy remove myminio $POLICY
    ${MC} admin policy add myminio $POLICY /tmp/$POLICY.json
  fi

  ${MC} admin policy set myminio $POLICY user=$USER
  echo "User '$USER' set policy: '$POLICY' "
}


# Try connecting to Minio instance
scheme=http
connectToMinio $scheme
createUser test test
allowBuckets test test
```

---

<h2 id="c-13-0" class="mh1">Docker Compose 部署 Seaweedfs </h2>

``` bash
services:
###
  seaweedfs-filer:
    image: ${SEAWEEDFS_V1_VERSION}
    restart: always
    command:
      - filer
      - -master=seaweedfs-manager:9333
      - -ip=0.0.0.0
      - -port=8888
      - -disableDirListing=true
    ports:
      - "8888:8888"
      - "18888:18888"
    volumes:
      - ${CONFIG_DIR}/seaweedfs-config:/etc/seaweedfs
      - ${SEAWEEDFS_DATA_DIR}/seaweedfs/filer:/data
    networks:
      fengren:
        ipv4_address: 172.18.0.68
  seaweedfs-manager:
    restart: always
    image: ${SEAWEEDFS_V1_VERSION}
    entrypoint:
      - /usr/bin/weed
      - master
      - -volumeSizeLimitMB=5000
      - -mdir=/data/
      - -peers=seaweedfs-manager:9333
      - -ip=seaweedfs-manager
      - -port=9333
    ports:
      - "9333:9333"
    volumes:
      - ${SEAWEEDFS_DATA_DIR}/seaweedfs/manager:/data
    networks:
      fengren:
        ipv4_address: 172.18.0.69
  seaweedfs-s3:
    image: ${SEAWEEDFS_V1_VERSION}
    restart: always
    command:
      - s3
      - -filer=seaweedfs-filer:8888
      - -port=8333
      - -config=/etc/seaweedfs/config.json
    ports:
      - "32631:8333"
    volumes:
      - ${CONFIG_DIR}/seaweedfs-config:/etc/seaweedfs
    networks:
      fengren:
        ipv4_address: 172.18.0.70
  seaweedfs-volume:
    restart: always
    image: ${SEAWEEDFS_V1_VERSION}
    entrypoint:
      - /usr/bin/weed
      - volume
      - -dir=/data/
      - -mserver=seaweedfs-manager:9333
      - -ip=seaweedfs-volume
      - -port=8080
      - -index=leveldbLarge
      - -max=10000
    volumes:
      - ${SEAWEEDFS_DATA_DIR}/seaweedfs/volume:/data
    networks:
      fengren:
        ipv4_address: 172.18.0.71
```

---

<h2 id="c-20-0" class="mh1">参考资源</h2>

- [kubernetes 最新版安装单机版 v1.21.5](https://blog.csdn.net/qq_14910065/article/details/122180162)
- [安装 Pod 网络插件（CNI）](https://blog.csdn.net/moxiaotang/article/details/124790965)

<hr aria-hidden="true" style=" border: 0; height: 2px; background: linear-gradient(90deg, transparent, #1bb75c, transparent); margin: 2rem 0; " />

<!-- 目录容器 -->
<div class="mi1">
    <strong>目录</strong>
        <ul style="margin: 10px 0; padding-left: 20px; list-style-type: none;">
            <li style="list-style-type: none;"><a href="#c-1-0">Mac 部署 Minikube</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-2-0">Centos7.9 部署 Kubeadmin</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-3-0">Centos7.9 部署 K8s 集群</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-4-0">Helm 部署 Kafka</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-5-0">Docker Compose 部署 RocketMQ</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-6-0">Docker Compose 部署 MQTT</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-7-0">Docker Compose 部署 TiDB</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-8-0">Docker Compose 部署 MySQL</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-9-0">Docker Compose 部署 NSQ</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-10-0">Docker Compose 部署 Redis</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-11-0">Docker Compose 部署 Vearch</a></li>
                <ul style="padding-left: 15px; list-style-type: none;">
                    <li style="list-style-type: none;"><a href="#c-11-1">vearch 部署</a></li>
                    <li style="list-style-type: none;"><a href="#c-11-2">查看集群中所有库</a></li>
                    <li style="list-style-type: none;"><a href="#c-11-3">查看库信息</a></li>
                    <li style="list-style-type: none;"><a href="#c-11-4">创建库</a></li>
                    <li style="list-style-type: none;"><a href="#c-11-5">查看表空间</a></li>
                    <li style="list-style-type: none;"><a href="#c-11-6">创建表空间</a></li>
                    <li style="list-style-type: none;"><a href="#c-11-7">id查询</a></li>
                    <li style="list-style-type: none;"><a href="#c-11-8">数据检索</a></li>
                </ul>
            <li style="list-style-type: none;"><a href="#c-12-0">Docker Compose 部署 MINIO</a></li>
                <ul style="padding-left: 15px; list-style-type: none;">
                    <li style="list-style-type: none;"><a href="#c-12-1">minio 部署</a></li>
                    <li style="list-style-type: none;"><a href="#c-12-2">权限初始化</a></li>
                </ul>
            <li style="list-style-type: none;"><a href="#c-13-0">Docker Compose 部署 Seaweedfs</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-20-0">参考资源</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
        </ul>
</div>

<style>
    /* 一级段落 */
    .mh1 {
      text-align: center;
      color: black;
      background: linear-gradient(#fff 60%, #b2e311ff 40%);
      margin: 1.4em 0 1.1em;
      font-size: 1.4em;
      font-family: 'roboto', 'Iowan Old Style', 'Ovo', 'Hoefler Text', Georgia, 'Times New Roman', 'TIBch', 'Source Han Sans', 'PingFangSC-Regular', 'Hiragino Sans GB', 'STHeiti', 'Microsoft Yahei', 'Droid Sans Fallback', 'WenQuanYi Micro Hei', sans-serif;
      line-height: 1.7;
      letter-spacing: .33px;
    }
    /* 二级段落 */

    .mh2 {
      -webkit-text-size-adjust: 100%; letter-spacing: .33px; font-family: 'roboto', 'Iowan Old Style', 'Ovo', 'Hoefler Text', Georgia, 'Times New Roman', 'TIBch', 'Source Han Sans', 'PingFangSC-Regular', 'Hiragino Sans GB', 'STHeiti', 'Microsoft Yahei', 'Droid Sans Fallback', 'WenQuanYi Micro Hei', sans-serif; line-height: 1.7; color: #1cc03cff; border-left: 4px solid #1bb75cff; padding-left: 6px; margin: 1.4em 0 1.1em;
    }

    /* 目录 高度、宽度 可自行调整*/
    .mi1 {
      position: fixed; bottom: 240px; right: 10px; width: 320px; height: 300px; background: #f8f9fa; border: 1px solid #e9ecef; border-radius: 8px; padding: 15px; overflow-y: auto; font-family: 'roboto', 'Iowan Old Style', 'Ovo', 'Hoefler Text', Georgia, 'Times New Roman', 'TIBch', 'Source Han Sans', 'PingFangSC-Regular', 'Hiragino Sans GB', 'STHeiti', 'Microsoft Yahei', 'Droid Sans Fallback', 'WenQuanYi Micro Hei', sans-serif; font-size: 14px; line-height: 1.15; color: #444; letter-spacing: 0.33px; transition: all 0.3s ease;
    }

</style>

本技术手册将持续更新，欢迎提交Issue和Pull Request
