---
layout: post
title: "K8s 安装部署教程"
date:   2024-8-30
tags: 
  - 软件类
comments: true
author: feng6917
---

K8s，懂得都懂，容器编排，部署，扩展，负载均衡等。

<!-- more -->

<h2 id="c-1-0" class="mh1">Mac 部署 Minikube</h2>

1. 安装 Home-brew <https://cloud.tencent.com/developer/article/1853162>

    ```shell
    > /usr/bin/ruby -e "$(curl -fsSL https://cdn.jsdelivr.net/gh/ineo6/homebrew-install/install)"
    ```

2. 安装 Minukube <https://minikube.sigs.k8s.io/docs/start/>

   ```shell
   > brew install minikube
   # 遇到问题: No such file or directory @ rb_sysopen - ...
   # 解决方式：https://zhuanlan.zhihu.com/p/491515480
   ```

3. 查看版本

    ```shell
    > kubectl version -o json # 显示版本信息
    ```

<h2 id="c-2-0" class="mh1">Centos7.9 部署 Kubeadmin</h2>

1. 查看版本号

   ```shell
   >  cat /etc/redhat-release
   ```

2. 添加 IP

   ```shell
   > ip add
   ```

3. 修改主机名称

   ```shell
   > hostnamectl set-hostname k8s-master && bash
   ```

4. 添加 hosts,这里的 IP 是自己服务器 ip

   ```shell
   > ifconfig #查看主机IP地址
     cat >> /etc/hosts << EOF
     x.x.x.x  k8s-master
     EOF
   ```

5. 关闭防火墙,关闭 selinux

   ```shell
   > systemctl stop firewalld
   > systemctl disable firewalld
   > sed -i 's/enforcing/disabled/' /etc/selinux/config # 永久
   > setenforce 0 # 临时
   ```

6. 关闭 swap

   ```shell
   > swapoff -a # 临时
   > sed -i 's/.*swap.*/#&/' /etc/fstab # 永久
   ```

7. 将桥接的 IPv4 流量传递到 iptables 的链

   ```shell
   > cat > /etc/sysctl.d/k8s.conf << EOF
     net.bridge.bridge-nf-call-ip6tables = 1
     net.bridge.bridge-nf-call-iptables = 1
     EOF

   > sysctl --system # 生效
   ```

8. 时间同步

   ```shell
   > yum install ntpdate -y
   > ntpdate time.windows.com
   ```

9. 安装 Docker

   ```shell
   > wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo
   > yum -y install docker-ce-20.10.12-3.el7
   > systemctl enable docker && systemctl start docker && systemctl status docker
   > docker --version

   ```

10. 给 docker 添加加速器

    ```shell
    > cat > /etc/docker/daemon.json << EOF
    {
      "registry-mirrors": ["https://qj799ren.mirror.aliyuncs.com"],
      "exec-opts": ["native.cgroupdriver=systemd"],
      "log-driver": "json-file",
      "log-opts": {
      "max-size": "100m"
    },
      "storage-driver": "overlay2"
    }
    EOF

    > systemctl restart docker

    ```

11. 添加 kubernetes 的 yum 源

    ```shell
    > cat > /etc/yum.repos.d/kubernetes.repo << EOF
      [kubernetes]
      name=Kubernetes
      baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
      enabled=1
      gpgcheck=0
      repo_gpgcheck=0
      gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
      https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
      EOF

    ```

12. 安装 kubeadm，kubelet 和 kubectl

    ```shell
    > yum -y install kubelet-1.21.5-0 kubeadm-1.21.5-0 kubectl-1.21.5-0  #当前时间最新版是v1.21.5固定版本，下面有用
    > systemctl enable kubelet
    ```

13. 部署 Kubernetes Master

    ```shell
    > kubeadm init --apiserver-advertise-address=10.0.4.11  --image-repository registry.aliyuncs.com/google_containers  --kubernetes-version v1.21.5  --service-cidr=10.96.0.0/12  --pod-network-cidr=10.244.0.0/16 --ignore-preflight-errors=all

    ```

- 参数说明：
  - –apiserver-advertise-address=10.0.4.11 这个参数就是 master 主机的 IP 地址，例如我的 Master 主机的 IP 是：10.0.4.11
  - –image-repository registry.aliyuncs.com/google_containers 这个是镜像地址，由于国外地址无法访问，故使用的阿里云仓库地址：repository registry.aliyuncs.com/google_containers
  - –kubernetes-version=v1.21.5 这个参数是下载的 k8s 软件版本号
  - –service-cidr=10.96.0.0/12 这个参数后的 IP 地址直接就套用 10.96.0.0/12 ,以后安装时也套用即可，不要更改
  - –pod-network-cidr=10.244.0.0/16 k8s 内部的 pod 节点之间网络可以使用的 IP 段，不能和 service-cidr 写一样，如果不知道怎么配，就先用这个 10.244.0.0/16
  - –ignore-preflight-errors=all 添加这个会忽略错误
- 执行语句后，看到如下的信息说明就安装成功了。
      ![img.png](img.png)  

14. 执行如下语句

    ```shell
    > mkdir -p $HOME/.kube
    > sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    > sudo chown $(id -u):$(id -g) $HOME/.kube/config
    > kubectl get nodes    #节点状态为NotReady
    ```

15. 安装 Pod 网络插件（CNI）

    ```shell
    > wget https://docs.projectcalico.org/archive/v3.20/manifests/calico.yaml
    ```

    ```
    ---
    # Source: calico/templates/calico-config.yaml
    # This ConfigMap is used to configure a self-hosted Calico installation.
    kind: ConfigMap
    apiVersion: v1
    metadata:
    name: calico-config
    namespace: kube-system
    data:
    # Typha is disabled.
    typha_service_name: &#34;none&#34;
    # Configure the backend to use.
    calico_backend: &#34;bird&#34;

    # Configure the MTU to use
    veth_mtu: &#34;1440&#34;

    # The CNI network configuration to install on each node.  The special
    # values in this config will be automatically populated.
    cni_network_config: |-
        {
        &#34;name&#34;: &#34;k8s-pod-network&#34;,
        &#34;cniVersion&#34;: &#34;0.3.1&#34;,
        &#34;plugins&#34;: [
            {
            &#34;type&#34;: &#34;calico&#34;,
            &#34;log_level&#34;: &#34;info&#34;,
            &#34;datastore_type&#34;: &#34;kubernetes&#34;,
            &#34;nodename&#34;: &#34;__KUBERNETES_NODE_NAME__&#34;,
            &#34;mtu&#34;: __CNI_MTU__,
            &#34;ipam&#34;: {
                &#34;type&#34;: &#34;calico-ipam&#34;
            },
            &#34;policy&#34;: {
                &#34;type&#34;: &#34;k8s&#34;
            },
            &#34;kubernetes&#34;: {
                &#34;kubeconfig&#34;: &#34;__KUBECONFIG_FILEPATH__&#34;
            }
            },
            {
            &#34;type&#34;: &#34;portmap&#34;,
            &#34;snat&#34;: true,
            &#34;capabilities&#34;: {&#34;portMappings&#34;: true}
            }
        ]
        }

    ---
    # Source: calico/templates/kdd-crds.yaml
    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: felixconfigurations.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: FelixConfiguration
        plural: felixconfigurations
        singular: felixconfiguration
    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: ipamblocks.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: IPAMBlock
        plural: ipamblocks
        singular: ipamblock

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: blockaffinities.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: BlockAffinity
        plural: blockaffinities
        singular: blockaffinity

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: ipamhandles.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: IPAMHandle
        plural: ipamhandles
        singular: ipamhandle

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: ipamconfigs.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: IPAMConfig
        plural: ipamconfigs
        singular: ipamconfig

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: bgppeers.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: BGPPeer
        plural: bgppeers
        singular: bgppeer

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: bgpconfigurations.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: BGPConfiguration
        plural: bgpconfigurations
        singular: bgpconfiguration

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: ippools.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: IPPool
        plural: ippools
        singular: ippool

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: hostendpoints.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: HostEndpoint
        plural: hostendpoints
        singular: hostendpoint

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: clusterinformations.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: ClusterInformation
        plural: clusterinformations
        singular: clusterinformation

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: globalnetworkpolicies.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: GlobalNetworkPolicy
        plural: globalnetworkpolicies
        singular: globalnetworkpolicy

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: globalnetworksets.crd.projectcalico.org
    spec:
    scope: Cluster
    group: crd.projectcalico.org
    version: v1
    names:
        kind: GlobalNetworkSet
        plural: globalnetworksets
        singular: globalnetworkset

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: networkpolicies.crd.projectcalico.org
    spec:
    scope: Namespaced
    group: crd.projectcalico.org
    version: v1
    names:
        kind: NetworkPolicy
        plural: networkpolicies
        singular: networkpolicy

    ---

    apiVersion: apiextensions.k8s.io/v1beta1
    kind: CustomResourceDefinition
    metadata:
    name: networksets.crd.projectcalico.org
    spec:
    scope: Namespaced
    group: crd.projectcalico.org
    version: v1
    names:
        kind: NetworkSet
        plural: networksets
        singular: networkset
    ---
    # Source: calico/templates/rbac.yaml

    # Include a clusterrole for the kube-controllers component,
    # and bind it to the calico-kube-controllers serviceaccount.
    kind: ClusterRole
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: calico-kube-controllers
    rules:
    # Nodes are watched to monitor for deletions.
    - apiGroups: [&#34;&#34;]
        resources:
        - nodes
        verbs:
            - watch
            - list
            - get
        # Pods are queried to check for existence.
        - apiGroups: [&#34;&#34;]
            resources:
            - pods
            verbs:
                - get
            # IPAM resources are manipulated when nodes are deleted.
            - apiGroups: [&#34;crd.projectcalico.org&#34;]
                resources:
                - ippools
                verbs:
                    - list
                - apiGroups: [&#34;crd.projectcalico.org&#34;]
                    resources:
                    - blockaffinities
                    - ipamblocks
                    - ipamhandles
                    verbs:
                        - get
                        - list
                        - create
                        - update
                        - delete
                    # Needs access to update clusterinformations.
                    - apiGroups: [&#34;crd.projectcalico.org&#34;]
                        resources:
                        - clusterinformations
                        verbs:
                            - get
                            - create
                            - update
    ---
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: calico-kube-controllers
    roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: calico-kube-controllers
    subjects:
    - kind: ServiceAccount
        name: calico-kube-controllers
        namespace: kube-system
    ---
    # Include a clusterrole for the calico-node DaemonSet,
    # and bind it to the calico-node serviceaccount.
    kind: ClusterRole
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
    name: calico-node
    rules:
    # The CNI plugin needs to get pods, nodes, and namespaces.
    - apiGroups: [&#34;&#34;]
        resources:
        - pods
        - nodes
        - namespaces
        verbs:
            - get
        - apiGroups: [&#34;&#34;]
            resources:
            - endpoints
            - services
            verbs:
                # Used to discover service IPs for advertisement.
                - watch
                - list
                # Used to discover Typhas.
                - get
            - apiGroups: [&#34;&#34;]
                resources:
                - nodes/status
                verbs:
                    # Needed for clearing NodeNetworkUnavailable flag.
                    - patch
                    # Calico stores some configuration information in node annotations.
                    - update
                # Watch for changes to Kubernetes NetworkPolicies.
                - apiGroups: [&#34;networking.k8s.io&#34;]
                    resources:
                    - networkpolicies
                    verbs:
                        - watch
                        - list
                    # Used by Calico for policy information.
                    - apiGroups: [&#34;&#34;]
                        resources:
                        - pods
                        - namespaces
                        - serviceaccounts
                        verbs:
                            - list
                            - watch
                        # The CNI plugin patches pods/status.
                        - apiGroups: [&#34;&#34;]
                            resources:
                            - pods/status
                            verbs:
                                - patch
                            # Calico monitors various CRDs for config.
                            - apiGroups: [&#34;crd.projectcalico.org&#34;]
                                resources:
                                - globalfelixconfigs
                                - felixconfigurations
                                - bgppeers
                                - globalbgpconfigs
                                - bgpconfigurations
                                - ippools
                                - ipamblocks
                                - globalnetworkpolicies
                                - globalnetworksets
                                - networkpolicies
                                - networksets
                                - clusterinformations
                                - hostendpoints
                                - blockaffinities
                                verbs:
                                    - get
                                    - list
                                    - watch
                                # Calico must create and update some CRDs on startup.
                                - apiGroups: [&#34;crd.projectcalico.org&#34;]
                                    resources:
                                    - ippools
                                    - felixconfigurations
                                    - clusterinformations
                                    verbs:
                                        - create
                                        - update
                                    # Calico stores some configuration information on the node.
                                    - apiGroups: [&#34;&#34;]
                                        resources:
                                        - nodes
                                        verbs:
                                            - get
                                            - list
                                            - watch
                                        # These permissions are only requried for upgrade from v2.6, and can
                                        # be removed after upgrade or on fresh installations.
                                        - apiGroups: [&#34;crd.projectcalico.org&#34;]
                                            resources:
                                            - bgpconfigurations
                                            - bgppeers
                                            verbs:
                                                - create
                                                - update
                                            # These permissions are required for Calico CNI to perform IPAM allocations.
                                            - apiGroups: [&#34;crd.projectcalico.org&#34;]
                                                resources:
                                                - blockaffinities
                                                - ipamblocks
                                                - ipamhandles
                                                verbs:
                                                    - get
                                                    - list
                                                    - create
                                                    - update
                                                    - delete
                                                - apiGroups: [&#34;crd.projectcalico.org&#34;]
                                                    resources:
                                                    - ipamconfigs
                                                    verbs:
                                                        - get
                                                    # Block affinities must also be watchable by confd for route aggregation.
                                                    - apiGroups: [&#34;crd.projectcalico.org&#34;]
                                                        resources:
                                                        - blockaffinities
                                                        verbs:
                                                            - watch
                                                        # The Calico IPAM migration needs to get daemonsets. These permissions can be
                                                        # removed if not upgrading from an installation using host-local IPAM.
                                                        - apiGroups: [&#34;apps&#34;]
                                                            resources:
                                                            - daemonsets
                                                            verbs:
                                                                - get
    ---
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
    name: calico-node
    roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: calico-node
    subjects:
    - kind: ServiceAccount
        name: calico-node
        namespace: kube-system

    ---
    # Source: calico/templates/calico-node.yaml
    # This manifest installs the calico-node container, as well
    # as the CNI plugins and network config on
    # each master and worker node in a Kubernetes cluster.
    kind: DaemonSet
    apiVersion: apps/v1
    metadata:
    name: calico-node
    namespace: kube-system
    labels:
        k8s-app: calico-node
    spec:
    selector:
        matchLabels:
        k8s-app: calico-node
    updateStrategy:
        type: RollingUpdate
        rollingUpdate:
        maxUnavailable: 1
    template:
        metadata:
        labels:
            k8s-app: calico-node
        annotations:
            # This, along with the CriticalAddonsOnly toleration below,
            # marks the pod as a critical add-on, ensuring it gets
            # priority scheduling and that its resources are reserved
            # if it ever gets evicted.
            scheduler.alpha.kubernetes.io/critical-pod: &#39;&#39;
        spec:
        nodeSelector:
            beta.kubernetes.io/os: linux
        hostNetwork: true
        tolerations:
            # Make sure calico-node gets scheduled on all nodes.
            - effect: NoSchedule
            operator: Exists
            # Mark the pod as a critical add-on for rescheduling.
            - key: CriticalAddonsOnly
            operator: Exists
            - effect: NoExecute
            operator: Exists
        serviceAccountName: calico-node
        # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a &#34;force
        # deletion&#34;: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
        terminationGracePeriodSeconds: 0
        priorityClassName: system-node-critical
        initContainers:
            # This container performs upgrade from host-local IPAM to calico-ipam.
            # It can be deleted if this is a fresh installation, or if you have already
            # upgraded to use calico-ipam.
            - name: upgrade-ipam
            image: calico/cni:v3.11.3
            command: [&#34;/opt/cni/bin/calico-ipam&#34;, &#34;-upgrade&#34;]
                env:
                - name: KUBERNETES_NODE_NAME
                    valueFrom:
                    fieldRef:
                        fieldPath: spec.nodeName
                - name: CALICO_NETWORKING_BACKEND
                    valueFrom:
                    configMapKeyRef:
                        name: calico-config
                        key: calico_backend
                volumeMounts:
                    - mountPath: /var/lib/cni/networks
                    name: host-local-net-dir
                    - mountPath: /host/opt/cni/bin
                    name: cni-bin-dir
                securityContext:
                    privileged: true
                # This container installs the CNI binaries
                # and CNI network config file on each node.
                - name: install-cni
                    image: calico/cni:v3.11.3
                    command: [&#34;/install-cni.sh&#34;]
                    env:
                        # Name of the CNI config file to create.
                        - name: CNI_CONF_NAME
                        value: &#34;10-calico.conflist&#34;
                        # The CNI network config to install on each node.
                        - name: CNI_NETWORK_CONFIG
                        valueFrom:
                            configMapKeyRef:
                            name: calico-config
                            key: cni_network_config
                        # Set the hostname based on the k8s node name.
                        - name: KUBERNETES_NODE_NAME
                        valueFrom:
                            fieldRef:
                            fieldPath: spec.nodeName
                        # CNI MTU Config variable
                        - name: CNI_MTU
                        valueFrom:
                            configMapKeyRef:
                            name: calico-config
                            key: veth_mtu
                        # Prevents the container from sleeping forever.
                        - name: SLEEP
                        value: &#34;false&#34;
                        volumeMounts:
                        - mountPath: /host/opt/cni/bin
                            name: cni-bin-dir
                        - mountPath: /host/etc/cni/net.d
                            name: cni-net-dir
                        securityContext:
                        privileged: true
                        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes
                        # to communicate with Felix over the Policy Sync API.
                        - name: flexvol-driver
                        image: calico/pod2daemon-flexvol:v3.11.3
                        volumeMounts:
                            - name: flexvol-driver-host
                            mountPath: /host/driver
                        securityContext:
                            privileged: true
                        containers:
                        # Runs calico-node container on each Kubernetes node.  This
                        # container programs network policy and routes on each
                        # host.
                        - name: calico-node
                            image: calico/node:v3.11.3
                            env:
                            # Use Kubernetes API as the backing datastore.
                            - name: DATASTORE_TYPE
                                value: &#34;kubernetes&#34;
                            # Wait for the datastore.
                            - name: WAIT_FOR_DATASTORE
                                value: &#34;true&#34;
                            # Set based on the k8s node name.
                            - name: NODENAME
                                valueFrom:
                                fieldRef:
                                    fieldPath: spec.nodeName
                            # Choose the backend to use.
                            - name: CALICO_NETWORKING_BACKEND
                                valueFrom:
                                configMapKeyRef:
                                    name: calico-config
                                    key: calico_backend
                            # Cluster type to identify the deployment type
                            - name: CLUSTER_TYPE
                                value: &#34;k8s,bgp&#34;
                            # Auto-detect the BGP IP address.
                            - name: IP
                                value: &#34;autodetect&#34;
                            # Enable IPIP
                            - name: CALICO_IPV4POOL_IPIP
                                value: &#34;Always&#34;
                            # Set MTU for tunnel device used if ipip is enabled
                            - name: FELIX_IPINIPMTU
                                valueFrom:
                                configMapKeyRef:
                                    name: calico-config
                                    key: veth_mtu
                            # The default IPv4 pool to create on startup if none exists. Pod IPs will be
                            # chosen from this range. Changing this value after installation will have
                            # no effect. This should fall within &#96;--cluster-cidr&#96;.
                            - name: CALICO_IPV4POOL_CIDR
                                value: &#34;10.244.0.0/16&#34;
                            # Disable file logging so &#96;kubectl logs&#96; works.
                            - name: CALICO_DISABLE_FILE_LOGGING
                                value: &#34;true&#34;
                            # Set Felix endpoint to host default action to ACCEPT.
                            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION
                                value: &#34;ACCEPT&#34;
                            # Disable IPv6 on Kubernetes.
                            - name: FELIX_IPV6SUPPORT
                                value: &#34;false&#34;
                            # Set Felix logging to &#34;info&#34;
                            - name: FELIX_LOGSEVERITYSCREEN
                                value: &#34;info&#34;
                            - name: FELIX_HEALTHENABLED
                                value: &#34;true&#34;
                            securityContext:
                            privileged: true
                            resources:
                            requests:
                                cpu: 250m
                            livenessProbe:
                            exec:
                                command:
                                - /bin/calico-node
                                - -felix-live
                                - -bird-live
                            periodSeconds: 10
                            initialDelaySeconds: 10
                            failureThreshold: 6
                            readinessProbe:
                            exec:
                                command:
                                - /bin/calico-node
                                - -felix-ready
                                - -bird-ready
                            periodSeconds: 10
                            volumeMounts:
                            - mountPath: /lib/modules
                                name: lib-modules
                                readOnly: true
                            - mountPath: /run/xtables.lock
                                name: xtables-lock
                                readOnly: false
                            - mountPath: /var/run/calico
                                name: var-run-calico
                                readOnly: false
                            - mountPath: /var/lib/calico
                                name: var-lib-calico
                                readOnly: false
                            - name: policysync
                                mountPath: /var/run/nodeagent
                        volumes:
                        # Used by calico-node.
                        - name: lib-modules
                            hostPath:
                            path: /lib/modules
                        - name: var-run-calico
                            hostPath:
                            path: /var/run/calico
                        - name: var-lib-calico
                            hostPath:
                            path: /var/lib/calico
                        - name: xtables-lock
                            hostPath:
                            path: /run/xtables.lock
                            type: FileOrCreate
                        # Used to install CNI.
                        - name: cni-bin-dir
                            hostPath:
                            path: /opt/cni/bin
                        - name: cni-net-dir
                            hostPath:
                            path: /etc/cni/net.d
                        # Mount in the directory for host-local IPAM allocations. This is
                        # used when upgrading from host-local to calico-ipam, and can be removed
                        # if not using the upgrade-ipam init container.
                        - name: host-local-net-dir
                            hostPath:
                            path: /var/lib/cni/networks
                        # Used to create per-pod Unix Domain Sockets
                        - name: policysync
                            hostPath:
                            type: DirectoryOrCreate
                            path: /var/run/nodeagent
                        # Used to install Flex Volume Driver
                        - name: flexvol-driver-host
                            hostPath:
                            type: DirectoryOrCreate
                            path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec/nodeagent~uds
    ---

    apiVersion: v1
    kind: ServiceAccount
    metadata:
    name: calico-node
    namespace: kube-system

    ---
    # Source: calico/templates/calico-kube-controllers.yaml

    # See https://github.com/projectcalico/kube-controllers
    apiVersion: apps/v1
    kind: Deployment
    metadata:
    name: calico-kube-controllers
    namespace: kube-system
    labels:
        k8s-app: calico-kube-controllers
    spec:
    # The controllers can only have a single active instance.
    replicas: 1
    selector:
        matchLabels:
        k8s-app: calico-kube-controllers
    strategy:
        type: Recreate
    template:
        metadata:
        name: calico-kube-controllers
        namespace: kube-system
        labels:
            k8s-app: calico-kube-controllers
        annotations:
            scheduler.alpha.kubernetes.io/critical-pod: &#39;&#39;
        spec:
        nodeSelector:
            beta.kubernetes.io/os: linux
        tolerations:
            # Mark the pod as a critical add-on for rescheduling.
            - key: CriticalAddonsOnly
            operator: Exists
            - key: node-role.kubernetes.io/master
            effect: NoSchedule
        serviceAccountName: calico-kube-controllers
        priorityClassName: system-cluster-critical
        containers:
            - name: calico-kube-controllers
            image: calico/kube-controllers:v3.11.3
            env:
                # Choose which controllers to run.
                - name: ENABLED_CONTROLLERS
                value: node
                - name: DATASTORE_TYPE
                value: kubernetes
            readinessProbe:
                exec:
                command:
                    - /usr/bin/check-status
                    - -r
    ---
    apiVersion: v1
    kind: ServiceAccount
    metadata:
    name: calico-kube-controllers
    namespace: kube-system
    ---
    # Source: calico/templates/calico-etcd-secrets.yaml
    ---
    # Source: calico/templates/calico-typha.yaml
    ---
    # Source: calico/templates/configure-canal.yaml
    EOF
    ```

    注意唯一需要修改的是 CALICO_IPV4POOL_CIDR 对应的 IP，需要与前面 kubeadm init 的 --pod-network-cidr 指定的一样。–pod-network-cidr=10.244.0.0/16

    ```shell
    > kubectl apply -f calico.yaml
    ```

16. 验证网络

    ```shell
    > kubectl get nodes  #看到 Ready说明网络就正常了
    > kubectl get pods -n kube-system  #全部显示Running就是对的
    ```

17. 单集版的 k8s 安装后, 无法部署服务

    ```shell
    # 因为默认master不能部署pod,有污点, 需要去掉污点或者新增一个node，我这里是去除污点。
    > kubectl get node -o yaml | grep taint -A 5 #执行后看到有输出说明有污点
    > kubectl taint nodes --all node-role.kubernetes.io/master-   #执行这句就行，就是取消污点
    ```

18. 安装补全命令的包

    ```shell
    > yum -y install bash-completion  #安装补全命令的包
    > kubectl completion bash
    > source /usr/share/bash-completion/bash_completion
    > kubectl completion bash >/etc/profile.d/kubectl.sh
    > source /etc/profile.d/kubectl.sh
    > cat  >>  /root/.bashrc <<EOF
    source /etc/profile.d/kubectl.sh
    EOF
    ```

19. 测试 kubernetes 集群

    ```shell
    # 在Kubernetes集群中部署一个Nginx：
    > kubectl create deployment nginx --image=nginx
    > kubectl expose deployment nginx --port=80 --type=NodePort
    > kubectl get pods,svc
    # 注意：看到对外暴露的是3xxxx端口,开放安全组端口范围
    ```

> centos 安装 k8s 基本参考 【kubernetes 最新版安装单机版 v1.21.5】，之所以 cpoy 一遍文章内容，主要担心原文丢失！！

---

<h2 id="c-3-0" class="mh1">Centos7.9 部署 K8s 集群</h2>

>
> 该安装包对系统初始化、K8s初始化、部署Docker、部署K8s Master、部署K8s Node 等进行了封装，见闻知意，直接使用即可。
> 通过遵循以下步骤，便可轻松部署Kubernetes集群。如有任何安装问题，请前往bilibil中，搜索：AI-Linker 进行问题留言！之后会更新更多精彩内容。还请多多关注！(文字摘要自安装包部署说明)

##### 安装文档

本文档旨在帮助初级开发者快速安装企业级的Kubernetes（k8s）集群。您无需进行复杂配置，只需简单设置服务器相关信息即可。

##### 安装条件

- 本安装包仅适配 CentOS 7.9、CentOS 7.6 以及 银河麒麟高级服务器操作系统V10 SP2。其他操作系统可能无法成功安装。
- 安装操作系统时请选择标准安装，最小化安装可能缺少必需的组件。
- 安装完成后，请将服务器IP地址设置为静态，以避免重启后IP变动引起的集群不可用问题。

##### 安装步骤

1. **修改配置文件**： 修改install_host 文件，参考如下修改示例:

   ```toml
   # 该示例讲解了，如何搭建一个三节点的k8s集群
   [all] #这里配置所有 机器的名称 和 机器的ip
   k8s-master ansible_host=192.168.211.110
   k8s-node1  ansible_host=192.168.211.120
   k8s-node2  ansible_host=192.168.211.130
   
   [all:vars] # 这里配置机器的账户密码，为了方便，所有机器账户密码需要保持一致
   ansible_ssh_pass=k8s@2024
   ansible_ssh_port=22
   ansible_ssh_user=root
   
   [master] # 这里配置master节点的名称，名称就是上面all中填写的名称
   k8s-master
   
   [node] # 这里配置node节点的名称，名称就是上面all中填写的名称
   k8s-node1
   k8s-node2
   ```

   这里的配置中包括了主节点（master）和工作节点（node）的设置。

2. **执行安装命令**： 在服务器上打开终端，运行以下命令以开始安装：

   ```shell
   ./install.sh
   ```

3. **检查安装状态**： 安装完成后，检查日志中各节点的状态。若显示 `failed=0`，则表示集群安装成功：

   ```shell
   PLAY RECAP********************************************************************
   k8s-master                 : ok=58   changed=56   unreachable=0    failed=0    skipped=5    rescued=0    ignored=1
   k8s-node1                  : ok=55   changed=53   unreachable=0    failed=0    skipped=4    rescued=0    ignored=1
   k8s-node2                  : ok=55   changed=53   unreachable=0    failed=0    skipped=4    rescued=0    ignored=1
   ```

[下载地址](通过百度网盘分享的文件：cluster-installer.zip
链接：<https://pan.baidu.com/s/17szuBvV3McxvC6O81BahjQ?pwd=t474>
提取码：t474)

<h2 id="c-4-0" class="mh1">Helm 部署 Kafka </h2>

1. 镜像下载，可通过链接下载kafka3.5，也可以通过<https://docker.aityp.com/> 搜索kafka、kafka-ui进行下载

2. 创建pv、pvc

   1. 创建pv、pvc yaml 文件，参考如下：

        ```
        apiVersion: v1
        kind: PersistentVolume
        metadata:
        name: kafka
        spec:
        claimRef:
            name: kafka
            namespace: kafka # 自定义命名空间
        capacity:
            storage: 3Ti # 自定义存储空间
        volumeMode: Filesystem
        accessModes:
            - ReadWriteOnce
        persistentVolumeReclaimPolicy: Retain
        storageClassName: local-storage
        local:
            path: /data/localpv/kafka # 自定义存储路径
        nodeAffinity:
            required:
            nodeSelectorTerms:
                - matchExpressions:
                    - key: kubernetes.io/hostname
                    operator: In
                    values:
                        - "k8s-master-7" # 自定义节点名称
        ---
        kind: PersistentVolumeClaim
        apiVersion: v1
        metadata:
        name: kafka
        namespace: kafka # 自定义命名空间
        spec:
        accessModes:
            - ReadWriteOnce
        resources:
            requests:
            storage: 3Ti # 自定义存储空间
        storageClassName: local-storage 
        ```

   2. 创建本地storage

      ```shell
      mkdir -p /data/localpv/kafka
      chmode 777 /data/localpv/kafka
      ```

   3. 执行命令创建pv、pvc

      ```shell
      kubectl apply -f kafka-pv.yaml
      ```

3. 创建kafka、kafka-ui
    1. kafka

        - kafka/Chart.yaml

            ```
            apiVersion: v2
            name: kafka
            description: A Helm chart for Kafka
            type: application
            version: 0.1.0
            appVersion: "1.0"
            ```

        - kafka/templates/services.yaml

            ```
            apiVersion: v1
            kind: Service
            metadata:
            name: {{ .Values.service.name }}
            labels:
                k8s-app: {{ .Release.Name }}
            spec:
            type: {{ .Values.service.type }}
            ports:
                {{- toYaml .Values.service.ports | nindent 4 }}
            selector:
                k8s-app: {{ .Release.Name }}
            ```

        - kafka/templates/statefulsets.yaml

            ```
            apiVersion: apps/v1
            kind: StatefulSet
            metadata:
            name: {{ .Release.Name }}
            labels:
                k8s-app: {{ .Release.Name }}
            spec:
            serviceName: {{ .Release.Name }}
            replicas: {{ .Values.statefulSet.replicas }}
            selector:
                matchLabels:
                k8s-app: {{ .Release.Name }}
            template:
                metadata:
                labels:
                    k8s-app: {{ .Release.Name }}
                spec:
                containers:
                    - name: kafka
                    image: {{ .Values.statefulSet.imageRepository }}:{{ .Values.statefulSet.imageVersion }}
                    imagePullPolicy: {{.Values.statefulSet.imagePullPolicy}}
                    command:
                        - sh
                        - -c
                        - |
                        NODE_ID=${POD_NAME##*-} 
                        echo "broker.id=$NODE_ID" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "node.id=$NODE_ID" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        CONTROLLER_QUORUM="" 
                        for i in $(seq 0 $((${KAFKA_REPLICAS}-1))); do 
                            CONTROLLER_QUORUM="${CONTROLLER_QUORUM}${i}@kafka-${i}.kafka:9093," 
                        done 
                        CONTROLLER_QUORUM=$(echo $CONTROLLER_QUORUM | sed 's/,$//') 
                        echo "controller.quorum.voters=$CONTROLLER_QUORUM" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "advertised.listeners=PLAINTEXT://${HOST_IP}:30092" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "num.partitions=12" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "default.replication.factor=${KAFKA_REPLICAS}" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "log.retention.minutes=30" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "message.max.bytes=6000000" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "max.request.bytes=6100000" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "replica.fetch.max.bytes=6200000" >> /opt/bitnami/kafka/config/kraft/server.properties && 
                        echo "log.dirs=/bitnami/kafka" >> /opt/bitnami/kafka/config/kraft/server.properties &&  
                        if [ ! -f "/bitnami/kafka/meta.properties" ]; then
                            kafka-storage.sh format -t LelM2dIFQkiUFvXCEcqRWA -c /opt/bitnami/kafka/config/kraft/server.properties 
                        fi &&
                        exec kafka-server-start.sh /opt/bitnami/kafka/config/kraft/server.properties
                    ports:
                        {{- toYaml .Values.statefulSet.ports | nindent 12 }}
                    resources:
                        {{- toYaml .Values.resources | nindent 12 }}
                    env:
                        - name: KAFKA_ENABLE_KRAFT
                        value: "yes"
                        - name: KAFKA_CFG_PROCESS_ROLES
                        value: "broker,controller"
                        - name: KAFKA_CFG_CONTROLLER_LISTENER_NAMES
                        value: "CONTROLLER"
                        - name: KAFKA_CFG_LISTENERS
                        value: "PLAINTEXT://:9092,CONTROLLER://:9093"
                        - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
                        value: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
                        - name: KAFKA_KRAFT_CLUSTER_ID
                        value: "LelM2dIFQkiUFvXCEcqRWA"
                        - name: ALLOW_PLAINTEXT_LISTENER
                        value: "yes"
                        - name: KAFKA_HEAP_OPTS
                        value: -Xmx12G -Xms12G
                        - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
                        value: "true"
                        - name: KAFKA_REPLICAS
                        value: "{{ .Values.statefulSet.replicas }}"
                        - name: HOST_IP
                        valueFrom:
                            fieldRef:
                            fieldPath: status.hostIP
                        - name: POD_NAME
                        valueFrom:
                            fieldRef:
                            fieldPath: metadata.name
                    volumeMounts:
                        - mountPath: /bitnami/kafka
                        name: kafka
                nodeSelector:
                    zhst.com/kafka: "1"
                volumes:
                    - name: kafka
                    persistentVolumeClaim:
                        claimName: kafka
            ```

        - kafka/values.yaml

            ```
            statefulSet:
            replicas: 1
            imageRepository: kafka # 镜像
            imageVersion: 3.5.0 # 镜像 tag
            imagePullPolicy: IfNotPresent
            ports:
            - name: kafka
                containerPort: 9092
                protocol: TCP
            - name: kafka-ctl
                containerPort: 9093
                protocol: TCP

            service:
            name: kafka
            type: NodePort
            ports:
            - port: 9092
                targetPort: 9092
                nodePort: 30092
                protocol: TCP
                name: kafka
            - port: 9093
                targetPort: 9093
                nodePort: 30093
                protocol: TCP
                name: kafka-ctl

            resources:
            requests:
                cpu: 4
                memory: 16Gi
            limits:
                cpu: 8
                memory: 16Gi
            ```
  
    2. kafka-ui

        - kafka-ui/Chart.yaml

            ```
            apiVersion: v2
            name: kafkaui
            description: A Helm chart for Kubernetes

            # A chart can be either an 'application' or a 'library' chart.
            #
            # Application charts are a collection of templates that can be packaged into versioned archives
            # to be deployed.
            #
            # Library charts provide useful utilities or functions for the chart developer. They're included as
            # a dependency of application charts to inject those utilities and functions into the rendering
            # pipeline. Library charts do not define any templates and therefore cannot be deployed.
            type: application

            # This is the chart version. This version number should be incremented each time you make changes
            # to the chart and its templates, including the app version.
            # Versions are expected to follow Semantic Versioning (https://semver.org/)
            version: 0.1.0

            # This is the version number of the application being deployed. This version number should be
            # incremented each time you make changes to the application. Versions are not expected to
            # follow Semantic Versioning. They should reflect the version the application is using.
            # It is recommended to use it with quotes.
            appVersion: "1.16.0"
            ```

        - kafka-ui/templates/deployment.yaml

            ```
            apiVersion: apps/v1
            kind: Deployment
            metadata:
            name: {{.Release.Name}}
            spec:
            replicas: {{.Values.deployment.replicas}}
            selector:
                matchLabels:
                k8s-app: {{.Release.Name}}
            template:
                metadata:
                labels:
                    k8s-app: {{.Release.Name}}
                spec:
                containers:
                - name: kafka-ui
                    image: {{.Values.deployment.imageRepository}}:{{.Values.deployment.imageTag}}
                    imagePullPolicy: {{.Values.deployment.imagePullPolicy}}
                    ports:
                    - containerPort: 8080
                    env:
                    - name: KAFKA_CLUSTERS_0_NAME
                    value: kafkaCluster
                    - name: KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS
                    value: kafka:9092
                    - name: DYNAMIC_CONFIG_ENABLED
                    value: "true"
                    - name: TZ
                    value: Asia/Shanghai
            ```

        - kafka-ui/templates/service.yaml

            ```
            apiVersion: v1
            kind: Service
            metadata:
            name: {{.Release.Name}}
            spec:
            type: {{.Values.service.sepc.type}}
            selector:
                k8s-app: {{.Release.Name}}
            ports:
                {{- toYaml .Values.service.ports | nindent 4 }}
            ```

        - kafka-ui/values.yaml

            ```
            deployment:
            replicas: 1
            imageRepository: kafkaui # 镜像
            imageTag: 53a65 # 镜像版本
            imagePullPolicy: IfNotPresent

            service:
            sepc:
                type: NodePort
            ports:
            - name: http
                port: 8888
                targetPort: 8080
                nodePort: 30080
            ```

4. helmfile 部署服务

    - helmfile.yaml

        ```
        releases:

        # kafka
        - name: kafka
            namespace: kafka
            chart: "./kafka"
            values:
            - "./kafka/values.yaml"
            wait: false
            atomic: false

        # kafka-ui
        - name: kafka-ui
            namespace: kafka
            chart: "./kafka-ui"
            values:
            - "./kafka-ui/values.yaml"
            wait: false
            atomic: false
        ```

    - 执行 创建、移除

        ```
        helmfile -f helmfile.yaml apply
        helmfile -f helmfile.yaml delete
        ```

5. 访问服务

    - 访问地址：<http://localhost:30080>

---

<h2 id="c-5-0" class="mh1">Docker Compose 部署 RocketMQ </h2>

```
version: '3.8'
services:
  namesrv:
    image: apache/rocketmq:5.3.2
    container_name: rmqnamesrv
    ports:
      - 9876:9876
    networks:
      - rocketmq
    command: sh mqnamesrv
  broker:
    image: apache/rocketmq:5.3.2
    container_name: rmqbroker
    ports:
      - 10909:10909
      - 10911:10911
      - 10912:10912
    environment:
      - NAMESRV_ADDR=rmqnamesrv:9876
    depends_on:
      - namesrv
    networks:
      - rocketmq
    command: sh mqbroker
  proxy:
    image: apache/rocketmq:5.3.2
    container_name: rmqproxy
    networks:
      - rocketmq
    depends_on:
      - broker
      - namesrv
    ports:
      - 8080:8080
      - 8081:8081
    restart: on-failure
    environment:
      - NAMESRV_ADDR=rmqnamesrv:9876
    command: sh mqproxy
networks:
  rocketmq:
    driver: bridge
    ipam:
     config:
      - subnet: 172.31.1.0/24

```

---

<h2 id="c-6-0" class="mh1">Docker Compose 部署 MQTT </h2>

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: emqx
  labels:
    app: emqx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: emqx
  template:
    metadata:
      labels:
        app: emqx
    spec:
      containers:
      - name: emqx
        image: emqx/emqx:latest
        imagePullPolicy: Never
        ports:
        - containerPort: 1883   # MQTT 端口
        - containerPort: 8083   # WebSocket 端口
        - containerPort: 8084   # WSS 端口
        - containerPort: 8883   # MQTT SSL 端口
        - containerPort: 18083  # Dashboard 端口
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            memory: "256Mi"
            cpu: "250m"
---
apiVersion: v1
kind: Service
metadata:
  name: emqx
  labels:
    app: emqx
spec:
  type: LoadBalancer
  ports:
  - port: 1883
    targetPort: 1883
    protocol: TCP
    name: mqtt
  - port: 8083
    targetPort: 8083
    protocol: TCP
    name: websocket
  - port: 8084
    targetPort: 8084
    protocol: TCP
    name: wss
  - port: 8883
    targetPort: 8883
    protocol: TCP
    name: mqtt-ssl
  - port: 18083
    targetPort: 18083
    protocol: TCP
    name: dashboard
  selector:
    app: emqx

```

---

<h2 id="c-7-0" class="mh1">参考资源</h2>

- [kubernetes 最新版安装单机版 v1.21.5](https://blog.csdn.net/qq_14910065/article/details/122180162)
- [安装 Pod 网络插件（CNI）](https://blog.csdn.net/moxiaotang/article/details/124790965)

<hr aria-hidden="true" style=" border: 0; height: 2px; background: linear-gradient(90deg, transparent, #1bb75c, transparent); margin: 2rem 0; " />

<!-- 目录容器 -->
<div class="mi1">
    <strong>目录</strong>
        <ul style="margin: 10px 0; padding-left: 20px; list-style-type: none;">
            <li style="list-style-type: none;"><a href="#c-1-0">Mac 部署 Minikube</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-2-0">Centos7.9 部署 Kubeadmin</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-3-0">Centos7.9 部署 K8s 集群</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-4-0">Helm 部署 Kafka</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-5-0">Docker Compose 部署 RocketMQ</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-6-0">Docker Compose 部署 MQTT</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
            <li style="list-style-type: none;"><a href="#c-7-0">参考资源</a></li>
            <ul style="padding-left: 15px; list-style-type: none;"></ul>
        </ul>
</div>

<style>
    /* 一级段落 */
    .mh1 {
      text-align: center;
      color: black;
      background: linear-gradient(#fff 60%, #b2e311ff 40%);
      margin: 1.4em 0 1.1em;
      font-size: 1.4em;
      font-family: 'roboto', 'Iowan Old Style', 'Ovo', 'Hoefler Text', Georgia, 'Times New Roman', 'TIBch', 'Source Han Sans', 'PingFangSC-Regular', 'Hiragino Sans GB', 'STHeiti', 'Microsoft Yahei', 'Droid Sans Fallback', 'WenQuanYi Micro Hei', sans-serif;
      line-height: 1.7;
      letter-spacing: .33px;
    }
    /* 二级段落 */

    .mh2 {
      -webkit-text-size-adjust: 100%; letter-spacing: .33px; font-family: 'roboto', 'Iowan Old Style', 'Ovo', 'Hoefler Text', Georgia, 'Times New Roman', 'TIBch', 'Source Han Sans', 'PingFangSC-Regular', 'Hiragino Sans GB', 'STHeiti', 'Microsoft Yahei', 'Droid Sans Fallback', 'WenQuanYi Micro Hei', sans-serif; line-height: 1.7; color: #1cc03cff; border-left: 4px solid #1bb75cff; padding-left: 6px; margin: 1.4em 0 1.1em;
    }

    /* 目录 高度、宽度 可自行调整*/
    .mi1 {
      position: fixed; bottom: 240px; right: 10px; width: 240px; height: 150px; background: #f8f9fa; border: 1px solid #e9ecef; border-radius: 8px; padding: 15px; overflow-y: auto; font-family: 'roboto', 'Iowan Old Style', 'Ovo', 'Hoefler Text', Georgia, 'Times New Roman', 'TIBch', 'Source Han Sans', 'PingFangSC-Regular', 'Hiragino Sans GB', 'STHeiti', 'Microsoft Yahei', 'Droid Sans Fallback', 'WenQuanYi Micro Hei', sans-serif; font-size: 14px; line-height: 1.15; color: #444; letter-spacing: 0.33px; transition: all 0.3s ease;
    }

</style>

本技术手册将持续更新，欢迎提交Issue和Pull Request
